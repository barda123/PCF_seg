{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from mask_utils import show_image_with_masks,iou,symmetric_hausdorff_distance,mean_contour_distance,dsc,normalise_image\n",
    "\n",
    "from network_utils import gpu_memory_limit\n",
    "\n",
    "from MultiResUNet.MultiResUNet import MultiResUnet\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit how much GPU RAM can be allocated by this notebook... 8GB is 1/3 of available\n",
    "# gpu_memory_limit(5000)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph outputs...\n",
    "DataDir = './data/pericardial/wsx_round2/'\n",
    "\n",
    "splitDataFile = os.path.join(DataDir,'splitData.pickle')\n",
    "\n",
    "if os.path.isfile(splitDataFile):\n",
    "    splitData = pickle.load(open(splitDataFile,'rb'))\n",
    "    X, X_test, Y, Y_test,pxArea,pxArea_test,pxSpacing,pxSpacing_test = splitData\n",
    "    \n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    #load data - these files created by extract_dcm_for_wsx.ipynb\n",
    "    X = np.load(os.path.join(DataDir,'X.npy'))\n",
    "    Y = np.load(os.path.join(DataDir,'Y.npy')).astype('float')\n",
    "    pxArea = np.load(os.path.join(DataDir,'pxSize.npy'))\n",
    "    pxSpacing = np.sqrt(pxArea)\n",
    "\n",
    "    #ensure the shape is correct arrays saved were rank 3, so this changes to rank 4 (last dimension represents channels)\n",
    "    X = X.reshape([*X.shape,1])\n",
    "    Y = Y.reshape([*Y.shape,1])\n",
    "\n",
    "    #do train/test split!\n",
    "    splitData = train_test_split(X, Y, pxArea,pxSpacing, test_size=0.2,random_state=101)\n",
    "    pickle.dump(splitData,open(splitDataFile,'wb'))\n",
    "    #extract individual bits\n",
    "    X, X_test, Y, Y_test,pxArea,pxArea_test,pxSpacing,pxSpacing_test = splitData\n",
    "\n",
    "del splitData #as this variable is o longer required   \n",
    "\n",
    "M = X.shape[0]\n",
    "MTest = X_test.shape[0]\n",
    "imShape = (1,*X.shape[1:])\n",
    "\n",
    "PXSPACING = pickle.load(open(os.path.join('data','PXSPACING.pickle'),'rb'))\n",
    "PXAREA = np.product(PXSPACING)\n",
    "\n",
    "RESULTFILE = os.path.join('data','models','model_history.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "modelBaseName = 'mrunet_bayesian_2020-07-13_13:40' \n",
    "\n",
    "#directory for graph outputs.\n",
    "graphDir = os.path.join('graphs','performance',modelBaseName)\n",
    "if not os.path.isdir(graphDir):\n",
    "    os.makedirs(graphDir)\n",
    "\n",
    "#location of the actual saved model\n",
    "modelBaseName = os.path.join('data','models',modelBaseName)\n",
    "\n",
    "modelParamFile = modelBaseName + '.h5'\n",
    "modelArchitecture = modelBaseName + '.json'\n",
    "\n",
    "with open( modelArchitecture , 'r') as json_file:\n",
    "    model = model_from_json( json_file.read() )\n",
    "\n",
    "model.load_weights(modelParamFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS FOR DOING STOCHASTIC PREDICTIONS...\n",
    "\n",
    "#FIXMMEEEEEEEE make it so these can be called on arrays where M>1!!!!! BECAUSE THIS SUCKS\n",
    "\n",
    "def global_iou(predictions):\n",
    "    \n",
    "    '''takes the iou of multiple different segmentations'''\n",
    "    \n",
    "    intersection = np.min(predictions,axis=0).sum()\n",
    "    union = np.max(predictions,axis=0).sum()\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def global_dsc(predictions):\n",
    "    \n",
    "    N = predictions.shape[0]\n",
    "    numerator = N * np.min(predictions,axis=0).sum()\n",
    "    denominator = predictions.sum()\n",
    "    \n",
    "    return numerator/denominator\n",
    "    \n",
    "def mean_pairwise_iou(predictions):\n",
    "    \n",
    "    #all combinations of inputs\n",
    "    ious = [iou(a,b) for a,b in itertools.combinations(predictions,2)]\n",
    "    \n",
    "    return np.mean(ious)\n",
    "\n",
    "def mean_pairwise_dsc(predictions):\n",
    "    \n",
    "    #all combinations of samples, which will be axis 0\n",
    "    dscs = np.array([dsc(a,b) for a,b in itertools.combinations(predictions,2)])\n",
    "    \n",
    "    dscs[np.isnan(dscs)] = 0\n",
    "    \n",
    "    return np.mean(dscs)\n",
    "    \n",
    "def voxel_uncertainty(predictions):\n",
    "    \n",
    "    '''voxel-wise uncertainty as defined in Roy et al (2018)'''\n",
    "    \n",
    "    #strcture-and-voxel-wise uncertainty (compresses over the sample axis\n",
    "    feature_uncertainty = -np.sum(predictions*np.log(predictions),axis = 0)\n",
    "    #global uncertainty is the sum over the feature axis\n",
    "    global_uncertainty = np.sum(feature_uncertainty,axis=-1)\n",
    "    \n",
    "    return global_uncertainty\n",
    "    \n",
    "def mean_std_area(predictions):\n",
    "    \n",
    "    '''the area occupied by each segmented channel. outputs two array: mean and standard deviation\n",
    "    RETURNS ANSWERS IN PIXELS WHICH MUST BE RESCALED LATER!!!!!!\n",
    "    '''\n",
    "    #get the dims\n",
    "    N = predictions.shape[0]\n",
    "    nPixels = np.product(predictions.shape[1:-1])\n",
    "    nFeatures = predictions.shape[-1]\n",
    "    \n",
    "    #reshape array so that it is (N,pixels,features) and thrshold.\n",
    "    predictions = predictions.reshape((N,nPixels,nFeatures)) > 0.5\n",
    "    \n",
    "    #sum of voxels for each \n",
    "    areas = np.sum(predictions,axis = 1)\n",
    "    \n",
    "    #mean, returning a value for each segmentation channel\n",
    "    mu = np.mean(areas,axis=0)\n",
    "    sigma = np.std(areas,axis=0)\n",
    "    \n",
    "    return mu,sigma\n",
    "\n",
    "def predict_stochastic(model,N,X):\n",
    "    \n",
    "    '''draw and summarise multiple predictions from a model\n",
    "    Arguments:\n",
    "        model {a model, for example a Keras model, with a predict method} -- is assumed to have some stochastic component, i.e. multiple\n",
    "        N {int} -- the number of sample predictions to be drawn from the stochastic model\n",
    "        X {numpy array, probably float} -- assumed to be already consistent with inputs to the model. MUST ONLY BE A SINGLE IMAGE AND NOT MULTIPLE STACKED!!!!!\n",
    "        \n",
    "    Returns:\n",
    "        consensus {numpy array, boolean} -- pixelwise segmentation of x\n",
    "        also various floats, representing different metrics for uncertainty and the outputs.\n",
    "    '''\n",
    "    \n",
    "    #draw N predictions from the model over x\n",
    "    predictions = np.stack([model.predict(X) for n in range(N)],axis=0)\n",
    "    \n",
    "    #binarise\n",
    "    predictions = predictions\n",
    "    \n",
    "    consensus = np.mean(predictions,axis=0)>0.5 \n",
    "    \n",
    "    #metrics described in Roy et al...\n",
    "    uncertainty = voxel_uncertainty(predictions)\n",
    "    \n",
    "    mpDsc = mean_pairwise_dsc(predictions)\n",
    "    gDsc = global_dsc(predictions)\n",
    "    \n",
    "    mpIou = mean_pairwise_iou(predictions)\n",
    "    gIou = global_iou(predictions)\n",
    "    meanArea,stdArea = mean_std_area(predictions)\n",
    "    \n",
    "    return consensus,uncertainty,meanArea,stdArea,mpDsc,gDsc,mpIou,gIou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets go through a range of values for N, the number of samples drawn in making a prediction, before manually selecting an optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricNames = ['mean pairwise Dice coefficient',\n",
    "               'global Dice coefficient',\n",
    "               'mean pairwise IOU',\n",
    "               'global IOU'\n",
    "              ]\n",
    "\n",
    "trueNames = ['true IOU','true DSC']\n",
    "\n",
    "names = ['/'.join((m,t)) for t,m in itertools.product(trueNames,metricNames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRange = np.arange(3,21,1)\n",
    "nAttempts = 25#number of times to run analysis - captures variability in r values and mean IOU/DSC over attempts, rather than each instance...\n",
    "dscMean = np.zeros((*NRange.shape,nAttempts))\n",
    "uncertaintyMean = np.zeros((*NRange.shape,nAttempts))\n",
    "dscStd = np.zeros(NRange.shape)\n",
    "uncertaintyStd = np.zeros(NRange.shape)\n",
    "areaStdMean = np.zeros((*NRange.shape,nAttempts))\n",
    "areaStdStd = np.zeros(NRange.shape)\n",
    "\n",
    "# U = np.zeros((NRange.shape[0],nAttempts,*Y_test.shape))\n",
    "\n",
    "r = np.zeros((NRange.shape[0],len(names),nAttempts))\n",
    "\n",
    "for Nind,N in enumerate(NRange):\n",
    "    for attempt in range(nAttempts):\n",
    "        clear_output()\n",
    "        print('-'.join((str(N),str(attempt))))\n",
    "        predTest,uncertaintyTest,meanAreaTest,stdAreaTest,mpDscTest,gDscTest,mpIouTest,gIouTest = map(np.array,zip(*[predict_stochastic(model,N,x.reshape(imShape)) for x in X_test]))\n",
    "\n",
    "        predTest = predTest.reshape(*Y_test.shape)\n",
    "        uncertaintyTest = uncertaintyTest.reshape(*Y_test.shape)\n",
    "        #loop over the example axis, calculating metrics for each image separately\n",
    "        TestDSC = [dsc(Y_test[m,:,:,:], predTest[m,:,:]) for m in range(MTest)]\n",
    "\n",
    "        #sum of entropy over segmented pixels\n",
    "        totalUncertainty = [np.sum(u) for u in uncertaintyTest.squeeze()]\n",
    "        \n",
    "        dscMean[Nind,attempt] = np.mean(TestDSC)\n",
    "        uncertaintyMean[Nind,attempt] = np.nanmean(totalUncertainty)\n",
    "\n",
    "        dscStd[Nind] = np.std(TestDSC)\n",
    "        uncertaintyStd[Nind] = np.std(totalUncertainty)\n",
    "\n",
    "        areaStdMean[Nind,attempt] = stdAreaTest.mean()\n",
    "        areaStdStd[Nind] = stdAreaTest.std()\n",
    "#         U[Nind,attempt,:,:,:,:] = uncertaintyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dscMean_Deterministic = 0.8005#from previous results with MRUNet/without any ...\n",
    "\n",
    "#set the graph x location for \n",
    "detX = NRange.max() + 2\n",
    "\n",
    "errorbarArgs = {'capsize':3,\n",
    "                'marker':'o'\n",
    "               }\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.errorbar(NRange,dscMean.mean(axis=-1),dscMean.std(axis=-1),**errorbarArgs)\n",
    "plt.ylabel('mean Dice')\n",
    "plt.axhline(dscMean_Deterministic,linestyle = '--',label = 'no dropout',c='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Monte Carlo N')\n",
    "plt.xticks(NRange)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.errorbar(NRange,areaStdMean.mean(axis=-1),areaStdMean.std(axis=-1),**errorbarArgs)\n",
    "plt.xlabel('Monte Carlo N')\n",
    "plt.ylabel('Mean estimated standard deviation of area (cm$^{2}$)')\n",
    "plt.xticks(NRange)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "# plt.errorbar(NRange,np.diff(uncertaintyMean.mean(axis=-1)),np.diff(uncertaintyMean.std(axis=-1)),**errorbarArgs)\n",
    "u = np.diff(uncertaintyMean,axis=0)\n",
    "plt.errorbar(NRange[1:],np.nanmean(u, axis=-1),np.nanstd(u,axis=-1))\n",
    "plt.xlabel('Monte Carlo N')\n",
    "plt.ylabel('Delta uncertainty')\n",
    "plt.xticks(NRange)\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join(graphDir,'sample_size_means.png'))\n",
    "plt.savefig(os.path.join(graphDir,'sample_size_means.svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so, the correct answer is to use a sample size which gives near-best performance _AND_ spread _AND_ r (if r is unchanged) (less samples is obvs better tho)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = 76\n",
    "\n",
    "im = X_test[eg].squeeze()\n",
    "\n",
    "negs = 3\n",
    "plt.figure(figsize = ((negs + 2)*5,5))\n",
    "plt.subplot(1,negs+2,1)\n",
    "plt.imshow(im,cmap = 'gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "\n",
    "preds = np.zeros((negs,*Y_test.shape[1:]))\n",
    "\n",
    "for ind in range(negs):\n",
    "    p = model.predict(im.reshape((1,208,208,1))) > 0.5\n",
    "    preds[ind,:,:,:] = p\n",
    "    \n",
    "    plt.subplot(1,negs+2,ind+2)\n",
    "    plt.imshow(preds[ind].squeeze(),cmap = 'gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.subplot(1,negs+2,negs+2)\n",
    "show_image_with_masks(im,[p.squeeze() for p in preds],[{'linewidth':1}]*negs)\n",
    "plt.savefig(os.path.join(graphDir, 'example_seg_mask' + str(eg) + '.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can quantify model performance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTest,uncertaintyTest,meanAreaTest,stdAreaTest,mpDscTest,gDscTest,mpIouTest,gIouTest = map(np.array,zip(*[predict_stochastic(model,N,x.reshape(imShape)) for x in X_test]))\n",
    "predTest = predTest.reshape(*Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loop over th eexample axis, calculating metrics for each image separately\n",
    "TestIOU = np.array([iou(Y_test[m,:,:,:], predTest[m,:,:]) for m in range(MTest)])\n",
    "TestDSC = np.array([dsc(Y_test[m,:,:,:], predTest[m,:,:]) for m in range(MTest)])\n",
    "TestHD = np.array([symmetric_hausdorff_distance(Y_test[m,:,:,:], predTest[m,:,:],pxSpacing_test[m]) for m in range(MTest)])\n",
    "TestMCD = np.array([mean_contour_distance(Y_test[m,:,:,:], predTest[m,:,:],pxSpacing_test[m]) for m in range(MTest)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at network performance.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms for each of the metrics...\n",
    "\n",
    "plt.figure(figsize = (20,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.hist(TestIOU ,  bins = np.arange(0,1.05,0.05), density=True, label = 'Test')\n",
    "plt.xlabel('Intersection-over-Union')\n",
    "plt.title(f'mean = {np.mean(TestIOU):.3f}, std = {np.std(TestIOU):.3f}')\n",
    "plt.ylabel('probability density')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.hist(TestDSC ,  bins = np.arange(0,1.05,0.05), density=True, label = 'Test')\n",
    "plt.xlabel('Dice score')\n",
    "plt.title(f'mean = {np.mean(TestDSC):.3f}, std = {np.std(TestDSC):.3f}')\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.hist(TestHD , bins = np.arange(0,125,5), density=True, label = 'Test')\n",
    "plt.xlabel('Hausdorff Distance (mm)')\n",
    "plt.title(f'mean = {np.mean(TestHD):.3f}, std = {np.std(TestHD):.3f}')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.hist(TestMCD , bins = np.arange(0,25,2), density=True, label = 'Test')\n",
    "plt.xlabel('Mean Contour Distance (mm)')\n",
    "plt.title(f'mean = {np.mean(TestMCD):.3f}, std = {np.std(TestMCD):.3f}')\n",
    "# plt.legend()\n",
    "\n",
    "plt.savefig(os.path.join(graphDir,'metric_histogram.png'))\n",
    "plt.savefig(os.path.join(graphDir,'metric_histogram.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bland-altman plot for the the test set...\n",
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "trueAreas = Y_test.sum(axis = (1,2)) * PXAREA/100\n",
    "predAreas = predTest.sum(axis = (1,2)) * PXAREA/100\n",
    "\n",
    "meanArea = (trueAreas + predAreas) /2\n",
    "diffArea = trueAreas - predAreas\n",
    "\n",
    "meanDiff = np.mean(diffArea)\n",
    "stdDiff = np.std(diffArea)\n",
    "\n",
    "plt.scatter(meanArea,diffArea)\n",
    "\n",
    "plt.axhline(meanDiff,c='k',alpha = 0.5)\n",
    "plt.axhline(meanDiff + 1.96*stdDiff,c='k',alpha = 0.5, linestyle = '--')\n",
    "plt.axhline(meanDiff - 1.96*stdDiff,c='k',alpha = 0.5, linestyle = '--')\n",
    "\n",
    "plt.title('auto vs manual (n = ' + str(MTest) + ')')\n",
    "\n",
    "plt.ylim([-40,40])\n",
    "plt.xlim([0,80])\n",
    "\n",
    "\n",
    "plt.savefig(os.path.join(graphDir,'bland_altman.png'))\n",
    "plt.savefig(os.path.join(graphDir,'bland_altman.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate a few metrics obviously r (already there), but also MAE for categries and MAE for the model predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for categorising DSC\n",
    "\n",
    "def categorise(metric,lowBoundary=0.6,highBoundary=0.8):\n",
    "    '''function for categorising DSC, returns an array of same dimension as input, formed of 0,1 and 2. This means that comparison of two such arrays is very simple'''\n",
    "    bad = metric < lowBoundary\n",
    "    good = metric >= highBoundary\n",
    "    medium = ~np.logical_or(bad,good)\n",
    "    onehot = np.stack((bad,medium,good), axis = -1)\n",
    "    categories = np.argmax(onehot, axis = -1)\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two ground-truths to be predicted\n",
    "\n",
    "#4 potential predictions to be made...\n",
    "plt.figure(figsize =(20,10))\n",
    "\n",
    "metrics = [mpDscTest,gDscTest,mpIouTest,gIouTest]\n",
    "metricNames = ['mean pairwise Dice coefficient',\n",
    "               'global Dice coefficient',\n",
    "               'mean pairwise IOU',\n",
    "               'global IOU'\n",
    "              ]\n",
    "\n",
    "def scatter_with_title(x,y,bounds):\n",
    "#     plt.plot([0,1],[0,1],c='k')\n",
    "    plt.scatter(x,y)\n",
    "    r,p = pearsonr(x,y)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x.reshape(-1,1),y)\n",
    "    pred = lr.predict(x.reshape(-1,1))\n",
    "    mae = np.mean(np.abs(pred-y))\n",
    "    \n",
    "    trueCat = categorise(y,*bounds)\n",
    "    predCat = categorise(pred,*bounds)\n",
    "    \n",
    "    catAcc = (trueCat==predCat).mean()\n",
    "    plt.title(f'r = {r:.3f}, MAE = {mae:.3f}\\ncategory accuracy = {catAcc:.3f}')\n",
    "    plt.axis('equal')\n",
    "    plt.plot([0,1],lr.predict(np.array([0,1]).reshape(-1,1)),c='k',label = 'linear fit')\n",
    "    plt.xlim([0.4,1])\n",
    "    plt.ylim([0.4,1])\n",
    "\n",
    "    \n",
    "for metricInd in range(4):\n",
    "    plt.subplot(2,4,metricInd+1)\n",
    "    scatter_with_title(metrics[metricInd],TestDSC,bounds=(0.6,0.8) )\n",
    "    if metricInd ==0:\n",
    "        plt.ylabel('True DSC')\n",
    "\n",
    "    plt.subplot(2,4,5+metricInd)\n",
    "    scatter_with_title(metrics[metricInd],TestIOU,bounds=(0.4,0.65) )\n",
    "    if metricInd ==0:\n",
    "        plt.ylabel('True IOU')\n",
    "    plt.xlabel(metricNames[metricInd])\n",
    "\n",
    "plt.savefig(os.path.join(graphDir,'QC_predictions_all_metrics.png'))\n",
    "plt.savefig(os.path.join(graphDir,'QC_predictions_all_metrics.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMetric = TestDSC.reshape(-1,1)#either DSC or IOU, depending on the above figures...\n",
    "bestOutput = mpDscTest.reshape(-1,1)#the best predictor of this metric, based on the above figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that this choice of metric/output is propagated to network_utils in order to facilitate scaleup of predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "convertModelFile = modelBaseName + '_prediction_conversion.pickle'\n",
    "if os.path.isfile(convertModelFile):\n",
    "    with open(convertModelFile,'rb') as f:\n",
    "        convertModel = pickle.load(f)\n",
    "else:\n",
    "    convertModel = LinearRegression()\n",
    "    convertModel.fit(bestOutput,bestMetric)\n",
    "    with open(convertModelFile,'wb') as fOut:\n",
    "        pickle.dump(convertModel,fOut)\n",
    "        \n",
    "predictedAccuracy = convertModel.predict(bestOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Roy et al, there are 3 categories of segmentation, bad (DSC < 0.6), medium (DSC >0.6,<0.8) and good (DSC > 0.8). So, lets look at the categories.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets have a look at what happens when we add noise as a general thing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to show r-values for both real images and those with artificially-introduced noise (see Roy et al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rician_noise(v,s):\n",
    "    \n",
    "    '''returns a rician-corrupted version of a single image'''\n",
    "    \n",
    "    x = np.random.normal(loc=0,scale=s,size=v.shape) + v\n",
    "    y = np.random.normal(loc=0,scale=s,size=v.shape)\n",
    "    \n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    return normalise_image(r)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = 0\n",
    "\n",
    "im = X_test[eg]\n",
    "\n",
    "signal = im.mean()\n",
    "\n",
    "SNRs = np.array([np.inf,4,3,2.5,2])\n",
    "\n",
    "ncols = SNRs.shape[0]+1\n",
    "\n",
    "plt.figure(figsize=(5*ncols,10))\n",
    "\n",
    "for i,SNR in enumerate(SNRs):\n",
    "    \n",
    "    scale = signal/SNR\n",
    "    \n",
    "    noise = add_rician_noise(im,scale)\n",
    "    \n",
    "    pred,uncertainty,meanArea,stdArea,mpDsc,gDsc,mpIou,gIou = predict_stochastic(model,N,noise.reshape(1,208,208,1))\n",
    "    pred = pred.squeeze()\n",
    "    \n",
    "    #show the image\n",
    "    plt.subplot(2,ncols,i+1)\n",
    "    show_image_with_masks(image = noise.squeeze(),\n",
    "                          masks = [Y_test[eg].squeeze(),pred],\n",
    "                          maskOptions = [{'linewidth':1,'color':'y'},{'linewidth':1,'color':'r'}]\n",
    "                         )\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if i==0:\n",
    "        plt.ylabel('image and segmentation')\n",
    "        uncertainties = uncertainty\n",
    "        plt.title('no added noise')\n",
    "    else:\n",
    "        uncertainties = np.concatenate((uncertainties,uncertainty),axis = 0)\n",
    "        plt.title('SNR = ' + str(SNR))\n",
    "    #color limits globally\n",
    "    cl = [0,uncertainties.max()]\n",
    "\n",
    "for i,SNR in enumerate(SNRs):\n",
    "    #pixelwise uncertainty\n",
    "    plt.subplot(2,ncols,ncols+1 + i)\n",
    "    plt.imshow(uncertainties[i].squeeze(),clim=cl)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if i==0:\n",
    "        plt.ylabel('pixelwise uncertainty')\n",
    "#dummy plot to allow colorbar\n",
    "plt.subplot(2,ncols,2*ncols )\n",
    "plt.imshow(np.zeros_like(uncertainties[i].squeeze()))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.savefig(os.path.join(graphDir,'noise_addition_example.png'))\n",
    "plt.savefig(os.path.join(graphDir,'noise_addition_example.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNRs = np.array([np.inf,4,3.5,3,2.5])\n",
    "\n",
    "ncols = SNRs.shape[0]+1\n",
    "\n",
    "plt.figure(figsize=(5*ncols,10))\n",
    "\n",
    "NoiseDSC = np.zeros(shape=(SNRs.shape[0],MTest))\n",
    "\n",
    "mpDscNoise = np.zeros_like(NoiseDSC)\n",
    "\n",
    "for i,SNR in enumerate(SNRs):\n",
    "    \n",
    "    signals = X_test.mean(axis=(1,2))\n",
    "    \n",
    "    scales = signals/SNR\n",
    "    \n",
    "    X_Noise = [add_rician_noise(im,scale) for im,scale in zip(X_test,scales)]\n",
    "    \n",
    "    predNoise,uncertaintyNoise,meanAreaNoise,stdAreaNoise,mpDscNoise[i,:],gDscNoise,mpIouNoise,gIouNoise = map(np.array,zip(*[predict_stochastic(model,N,x.reshape(imShape)) for x in X_Noise]))\n",
    "    predNoise = predNoise.reshape(*Y_test.shape)\n",
    "    #loop over theexample axis, calculating DSC for each image\n",
    "    NoiseDSC[i,:] = np.array([dsc(Y_test[m,:,:,:], predNoise[m,:,:]) for m in range(MTest)])\n",
    "    \n",
    "#     mpDscNoise[i,:] = mpDscNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mpDscNoise.flatten()\n",
    "y = NoiseDSC.flatten()\n",
    "\n",
    "nans = np.isnan(x)\n",
    "x = x[~nans].reshape(-1,1)\n",
    "y = y[~nans].reshape(-1,1)\n",
    "\n",
    "pred = convertModel.predict(x)\n",
    "\n",
    "r = pearsonr(y.flatten(),pred.flatten())[0]\n",
    "\n",
    "\n",
    "mae = np.mean(np.abs(pred-y))\n",
    "\n",
    "trueCat = categorise(y,*bounds)\n",
    "predCat = categorise(pred,*bounds)\n",
    "\n",
    "catAcc = (trueCat==predCat).mean()\n",
    "\n",
    "bounds = (0.6,0.8)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "\n",
    "plt.title(f'r = {r:.3f}, MAE = {mae:.3f}\\ncategory accuracy = {catAcc:.3f}')\n",
    "plt.axis('equal')\n",
    "plt.plot([0,1],convertModel.predict(np.array([0,1]).reshape(-1,1)),c='k',label = 'linear fit')\n",
    "for i,(p,t) in enumerate(zip(mpDscNoise,NoiseDSC)):\n",
    "    if i == 0:\n",
    "        label = 'no noise added'\n",
    "    else:\n",
    "        label = 'SNR = ' + str(SNRs[i])\n",
    "    plt.scatter(p,t,label = label)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negs = 8\n",
    "\n",
    "egs = np.random.choice(range(MTest), negs, replace=False)\n",
    "\n",
    "ncols = 4\n",
    "nrows = np.ceil(negs/ncols)\n",
    "\n",
    "plt.figure(figsize = (5*ncols,5*nrows))\n",
    "\n",
    "imShape = X_test.shape[1:-1]\n",
    "\n",
    "for i in range(negs):\n",
    "    \n",
    "    plt.subplot(nrows,ncols,i+1)\n",
    "    manual,automated = Y_test[egs[i],:,:].reshape(imShape), predTest[egs[i],:,:].reshape(imShape) > 0.5\n",
    "    \n",
    "    pxS = pxSpacing_test[egs[i]]\n",
    "    \n",
    "    show_image_with_masks(image = X_test[egs[i],:,:].reshape(imShape),\n",
    "                          masks = [manual,automated],\n",
    "                          maskOptions = [{'linewidth':1,'color':'y','label':'manual'},{'linewidth':1,'color':'r','label':'automated'}]\n",
    "                         )\n",
    "    \n",
    "    plt.title('Dice = ' + f'{bestMetric[egs[i]][0]:.03}' + '\\n' + \n",
    "              'predicted Dice = ' + f'{predictedAccuracy[egs[i]][0]:.03}'\n",
    "              )\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(os.path.join(graphDir,'test_examples.png'))\n",
    "plt.savefig(os.path.join(graphDir,'test_examples.svg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0 GPU",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
