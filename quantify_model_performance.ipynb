{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from mask_utils import show_image_with_masks,iou,symmetric_hausdorff_distance,mean_contour_distance,dsc\n",
    "\n",
    "from network_utils import gpu_memory_limit\n",
    "\n",
    "from MultiResUNet.MultiResUNet import MultiResUnet\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit how much GPU RAM can be allocated by this notebook... 8GB is 1/3 of available\n",
    "gpu_memory_limit(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataDir = './data/pericardial/wsx_round2/'\n",
    "\n",
    "#load data - these files created by extract_dcm_for_wsx.ipynb\n",
    "X = np.load(os.path.join(DataDir,'X.npy'))\n",
    "Y = np.load(os.path.join(DataDir,'Y.npy')).astype('float')\n",
    "pxArea = np.load(os.path.join(DataDir,'pxSize.npy'))\n",
    "pxSpacing = np.sqrt(pxArea)\n",
    "\n",
    "#ensure the shape is correct arrays saved were rank 3, so this changes to rank 4 (last dimension represents channels)\n",
    "X = X.reshape([*X.shape,1])\n",
    "Y = Y.reshape([*Y.shape,1])\n",
    "\n",
    "#do train/test split!\n",
    "X, X_test, Y, Y_test,pxArea,pxArea_test,pxSpacing,pxSpacing_test = train_test_split(X, Y, pxArea,pxSpacing, test_size=0.2,random_state=101)\n",
    "\n",
    "#\n",
    "M = X.shape[0]\n",
    "MTest = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "modelBaseName = 'mrunet_bayesian_2020-06-15_17:46' \n",
    "\n",
    "modelBaseName = os.path.join('data','models',modelBaseName)\n",
    "\n",
    "modelParamFile = modelBaseName + '.h5'\n",
    "modelArchitecture = modelBaseName + '.json'\n",
    "\n",
    "with open( modelArchitecture , 'r') as json_file:\n",
    "    model = model_from_json( json_file.read() )\n",
    "\n",
    "model.load_weights(modelParamFile)\n",
    "\n",
    "model_noDropout = MultiResUnet(height=X.shape[1],\n",
    "                               width=X.shape[2],\n",
    "                               n_channels=1,\n",
    "                               layer_dropout_rate=None,\n",
    "                               block_dropout_rate=0,\n",
    "                              ).load_weights(modelParamFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS FOR DOING STOCHASTIC PREDICTIONS...\n",
    "\n",
    "#FIXMMEEEEEEEE make it so these can be called on arrays where M>1!!!!! BECAUSE THIS SUCKS\n",
    "\n",
    "def global_iou(predictions):\n",
    "    \n",
    "    '''takes the iou of multiple different segmentations'''\n",
    "    \n",
    "    intersection = np.min(predictions,axis=0).sum()\n",
    "    union = np.max(predictions,axis=0).sum()\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def global_dsc(predictions):\n",
    "    \n",
    "    N = predictions.shape[0]\n",
    "    numerator = N * np.min(predictions,axis=0).sum()\n",
    "    denominator = predictions.sum()\n",
    "    \n",
    "    return numerator/denominator\n",
    "    \n",
    "def mean_pairwise_iou(predictions):\n",
    "    \n",
    "    #all combinations of inputs\n",
    "    ious = [iou(a,b) for a,b in itertools.combinations(predictions,2)]\n",
    "    \n",
    "    return np.mean(ious)\n",
    "\n",
    "def mean_pairwise_dsc(predictions):\n",
    "    \n",
    "    #all combinations of samples, which will be axis 0\n",
    "    dscs = [dsc(a,b) for a,b in itertools.combinations(predictions,2)]\n",
    "    \n",
    "    return np.mean(dscs)\n",
    "    \n",
    "def voxel_uncertainty(predictions):\n",
    "    \n",
    "    '''voxel-wise uncertainty as defined in Roy et al (2018)'''\n",
    "    \n",
    "    #strcture-and-voxel-wise uncertainty (compresses over the sample axis\n",
    "    feature_uncertainty = -np.sum(predictions*np.log(predictions),axis = 0)\n",
    "    #global uncertainty is the sum over the feature axis\n",
    "    global_uncertainty = np.sum(feature_uncertainty,axis=-1)\n",
    "    \n",
    "    return global_uncertainty\n",
    "    \n",
    "def mean_std_area(predictions):\n",
    "    \n",
    "    '''the area occupied by each segmented channel. outputs two array: mean and standard deviation\n",
    "    RETURNS ANSWERS IN PIXELS WHICH MUST BE RESCALED LATER!!!!!!\n",
    "    '''\n",
    "    #get the dims\n",
    "    N = predictions.shape[0]\n",
    "    nPixels = np.product(predictions.shape[1:-1])\n",
    "    nFeatures = predictions.shape[-1]\n",
    "    \n",
    "    #reshape array so that it is (N,pixels,features) and thrshold.\n",
    "    predictions = predictions.reshape((N,nPixels,nFeatures)) > 0.5\n",
    "    \n",
    "    #sum of voxels for each \n",
    "    areas = np.sum(predictions,axis = 1)\n",
    "    \n",
    "    #mean, returning a value for each segmentation channel\n",
    "    mu = np.mean(areas,axis=0)\n",
    "    sigma = np.std(areas,axis=0)\n",
    "    \n",
    "    return mu,sigma\n",
    "\n",
    "def predict_stochastic(model,N,X):\n",
    "    \n",
    "    '''draw and summarise multiple predictions from a model\n",
    "    Arguments:\n",
    "        model {a model, for example a Keras model, with a predict method} -- is assumed to have some stochastic component, i.e. multiple\n",
    "        N {int} -- the number of sample predictions to be drawn from the stochastic model\n",
    "        X {numpy array, probably float} -- assumed to be already consistent with inputs to the model. MUST ONLY BE A SINGLE IMAGE AND NOT MULTIPLE STACKED!!!!!\n",
    "        \n",
    "    Returns:\n",
    "        consensus {numpy array, boolean} -- pixelwise segmentation of x\n",
    "        also various floats, representing different metrics for uncertainty and the outputs.\n",
    "    '''\n",
    "    \n",
    "    #draw N predictions from the model over x\n",
    "    predictions = np.stack([model.predict(X) for n in range(N)],axis=0)\n",
    "    \n",
    "    #binarise\n",
    "    predictions = predictions\n",
    "    \n",
    "    consensus = np.mean(predictions,axis=0)>0.5 \n",
    "    \n",
    "    #metrics described in Roy et al...\n",
    "    uncertainty = voxel_uncertainty(predictions)\n",
    "    \n",
    "    mpDsc = mean_pairwise_dsc(predictions)\n",
    "    gDsc = global_dsc(predictions)\n",
    "    \n",
    "    mpIou = mean_pairwise_iou(predictions)\n",
    "    gIou = global_iou(predictions)\n",
    "    meanArea,stdArea = mean_std_area(predictions)\n",
    "    \n",
    "    return consensus,uncertainty,meanArea,stdArea,mpDsc,gDsc,mpIou,gIou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets go through a range of values for N, the number of samples drawn in making a prediction, before manually selecting an optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRange = np.arange(2,12,2)\n",
    "\n",
    "iouMean = np.zeros_like(NRange)\n",
    "dscMean = np.zeros_like(NRange)\n",
    "iouStd = np.zeros_like(NRange)\n",
    "dscStd = np.zeros_like(NRange)\n",
    "areaStdMean = np.zeros_like(NRange)\n",
    "areaStdStd = np.zeros_like(NRange)\n",
    "\n",
    "for ind,N in enumerate(NRange):\n",
    "\n",
    "    predTest,uncertaintyTest,meanAreaTest,stdAreaTest,mpDscTest,gDscTest,mpIouTest,gIouTest = map(np.array,zip(*[predict_stochastic(model,N,x.reshape(1,208,208,1)) for x in X_test]))\n",
    "    \n",
    "    predTest = predTest.reshape(*Y_test.shape)\n",
    "    #loop over th eexample axis, calculating metrics for each image separately\n",
    "    TestIOU = [iou(Y_test[m,:,:,:], predTest[m,:,:]) for m in range(MTest)]\n",
    "    TestDSC = [dsc(Y_test[m,:,:,:], predTest[m,:,:]) for m in range(MTest)]\n",
    "    \n",
    "    iouMean[ind] = np.mean(TestIOU)\n",
    "    dscMean[ind] = np.mean(TestDSC)\n",
    "    \n",
    "    iouStd[ind] = np.std(TestIOU)\n",
    "    dscStd[ind] = np.std(TestDSC)\n",
    "    \n",
    "    areaStdMean[ind] = stdAreaTest.mean()\n",
    "    areaStdStd[ind] =stdAreaTest.std()\n",
    "\n",
    "#get the benchmark for model without dropout...\n",
    "predDeterministic = model_noDropout.predict(X_test)\n",
    "#loop over th eexample axis, calculating metrics for each image separately\n",
    "TestIOU_Deterministic = [iou(Y_test[m,:,:,:], predDeterministic[m,:,:]) for m in range(MTest)]\n",
    "TestDSC_Deterministic = [dsc(Y_test[m,:,:,:], predDeterministic[m,:,:]) for m in range(MTest)]\n",
    "\n",
    "iouMean_Deterministic = np.mean(TestIOU_Deterministic)\n",
    "dscMean_Deterministic = np.mean(TestDSC_Deterministic)\n",
    "iouStd_Deterministic = np.std(TestIOU_Deterministic)\n",
    "dscStd_Deterministic = np.std(TestDSC_Deterministic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#set the graph x location for \n",
    "detX = NRange.max() + 2\n",
    "\n",
    "errorbarArgs = {'capsize':3,\n",
    "                'marker':'o'\n",
    "               }\n",
    "\n",
    "xt = np.arange(NRange.min(),NRange.max()+1,5)\n",
    "xTicks = xt.tolist().append(detX)\n",
    "xNames = xt.tolist().append('No Dropout')\n",
    "xTickLabels = dict(zip(xTicks,xNames))\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.errorbar(NRange,iouMean,iouStd,**errorbarArgs)\n",
    "plt.errorbar(detX,iouMean_Deterministic,iouStd_Deterministic,**errorbarArgs)\n",
    "plt.xlabel('Monte Carlo N')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.errorbar(NRange,dscMean,dscStd)\n",
    "plt.errorbar(detX,dscMean_Deterministic,dscStd_Deterministic)\n",
    "plt.xlabel('Monte Carlo N')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.errorbar(NRange,areaStdMean,areaStdStd)\n",
    "plt.xlabel('Monte Carlo N')\n",
    "plt.ylabel()\n",
    "\n",
    "plt.savefig(modelBaseName + '_sample_size.png')\n",
    "plt.savefig(modelBaseName + '_sample_size.svg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so, the correct answer is to use a sample size which gives near-best performance _AND_ spread (less samples is obvs better tho)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "predTest,uncertaintyTest,meanAreaTest,stdAreaTest,mpDscTest,gDscTest,mpIouTest,gIouTest = map(np.array,zip(*[predict_stochastic(model,N,x.reshape(1,208,208,1)) for x in X_test]))\n",
    "\n",
    "predTest = predTest.reshape(*Y_test.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0 GPU",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
