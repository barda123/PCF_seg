{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook to be used for developing the Bayesian framework of Roy et al (2018) in combination with the MultiResUNet architecture, such that uncertainty in segmentations can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# from custom_losses import binary_crossentropy_weight_balance, binary_crossentropy_weight_dict, binary_crossentropy_closeness_to_foreground,dice_coef_loss\n",
    "\n",
    "from mask_utils import show_image_with_masks,iou,symmetric_hausdorff_distance,mean_contour_distance,dsc\n",
    "\n",
    "from network_utils import gpu_memory_limit,augmentImageSequence\n",
    "\n",
    "from MultiResUNet.MultiResUNet import MultiResUnet\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit how much GPU RAM can be allocated by this notebook... 8GB is 1/3 of available\n",
    "gpu_memory_limit(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory for keeping models, and journaling their performance/graphs\n",
    "modelDir = os.path.join('data','models')\n",
    "if not os.path.isdir(modelDir):\n",
    "    os.mkdir(modelDir)\n",
    "    \n",
    "dateStr = datetime.now().strftime('%Y-%m-%d_%H:%M')\n",
    "outputName = os.path.join(modelDir,'mrunet_bayesian_' + dateStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataDir = './data/pericardial/wsx_round2/'\n",
    "\n",
    "#load data - these files created by extract_dcm_for_wsx.ipynb\n",
    "X = np.load(os.path.join(DataDir,'X.npy'))\n",
    "Y = np.load(os.path.join(DataDir,'Y.npy')).astype('float')\n",
    "pxArea = np.load(os.path.join(DataDir,'pxSize.npy'))\n",
    "pxSpacing = np.sqrt(pxArea)\n",
    "\n",
    "#ensure the shape is correct arrays saved were rank 3, so this changes to rank 4 (last dimension represents channels)\n",
    "X = X.reshape([*X.shape,1])\n",
    "Y = Y.reshape([*Y.shape,1])\n",
    "\n",
    "#do train/test split!\n",
    "X, X_test, Y, Y_test,pxArea,pxArea_test,pxSpacing,pxSpacing_test = train_test_split(X, Y, pxArea,pxSpacing, test_size=0.2,random_state=101)\n",
    "\n",
    "#\n",
    "M = X.shape[0]\n",
    "MTest = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#properties for data augmentation - that does nothing except randomise the order\n",
    "# dataGenArgs = dict(rotation_range=0,\n",
    "#                    width_shift_range=0,\n",
    "#                    height_shift_range=0,\n",
    "#                    shear_range=0,#0.05,\n",
    "#                    zoom_range=0,\n",
    "#                    horizontal_flip=False,\n",
    "#                    vertical_flip=False,\n",
    "#                    fill_mode='nearest',\n",
    "#                    data_format= 'channels_last',\n",
    "#                    featurewise_center=False,\n",
    "#                    featurewise_std_normalization=False,\n",
    "#                    zca_whitening=False,\n",
    "#                   )\n",
    "\n",
    "# #REAL properties for data augmentation\n",
    "dataGenArgs = dict(rotation_range=10,\n",
    "                   width_shift_range=0.1,\n",
    "                   height_shift_range=0.1,\n",
    "                   shear_range=0.05,\n",
    "                   zoom_range=0.1,\n",
    "                   horizontal_flip=False, #DO NOT FLIP THE IMAGES FFS\n",
    "                   vertical_flip=False,\n",
    "                   fill_mode='nearest',\n",
    "                   data_format= 'channels_last',\n",
    "                   featurewise_center=False,\n",
    "                   featurewise_std_normalization=False,\n",
    "                   zca_whitening=False,\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "earlyStop = callbacks.EarlyStopping(patience=10, #be a bit patient...\n",
    "                                    min_delta=0,\n",
    "                                    monitor='loss',\n",
    "                                    restore_best_weights=True,\n",
    "                                    mode='min',\n",
    "                                   )\n",
    "\n",
    "reduceLR = callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       patience=5,\n",
    "                                       factor=0.3,\n",
    "                                       verbose=1,\n",
    "                                       cooldown=5,\n",
    "                                      )\n",
    "\n",
    "CALLBACKS = [earlyStop,\n",
    "             reduceLR\n",
    "            ]\n",
    "\n",
    "OPT = Adam(learning_rate = 1e-2,\n",
    "           beta_1 = 0.9,\n",
    "           beta_2 = 0.999,\n",
    "           amsgrad = False\n",
    "          )\n",
    "\n",
    "#other hyperparameters\n",
    "BATCHSIZE = 8 #THIS MATTERS A LOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.clear_session()\n",
    "\n",
    "tf.random.set_seed(101) #FIXME!!! this is not sufficient to guarantee deterministic behaviour during fitting.\n",
    "\n",
    "model = MultiResUnet(height=X.shape[1],\n",
    "                     width=X.shape[2],\n",
    "                     n_channels=1,\n",
    "                     layer_dropout_rate=None,\n",
    "                     block_dropout_rate=0.5\n",
    "                    )\n",
    "\n",
    "model.compile(optimizer = OPT, \n",
    "#               loss = 'binary_crossentropy',\n",
    "#               loss = binary_crossentropy_weight_balance,\n",
    "#               loss = binary_crossentropy_closeness_to_foreground(sigma=SIGMA),\n",
    "              loss = dice_coef_loss,\n",
    "              metrics = ['accuracy',metrics.MeanIoU(num_classes=2)],\n",
    "             )\n",
    "\n",
    "fitHistory = model.fit(augmentImageSequence(X,Y,dataGenArgs,batchSize=BATCHSIZE),\n",
    "                       epochs = 50,#think about me... \n",
    "                       steps_per_epoch= M//BATCHSIZE, #obvs\n",
    "#                        workers=2,\n",
    "                       use_multiprocessing=False,\n",
    "                       validation_data=(X_test,Y_test.astype('float')),\n",
    "                       callbacks=CALLBACKS,\n",
    "                       verbose=1,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at how fitting has proceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(fitHistory.history['loss'],label = 'train')\n",
    "plt.plot(fitHistory.history['val_loss'],label = 'dev')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xticks([])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(fitHistory.history['mean_io_u'],label = 'train')\n",
    "plt.plot(fitHistory.history['val_mean_io_u'],label = 'dev')\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel('mean iou')\n",
    "\n",
    "plt.xlabel('epoch #')\n",
    "\n",
    "plt.savefig(outputName + '_loss_history.svg')\n",
    "plt.savefig(outputName + '_loss_history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS FOR DOING STOCHASTIC PREDICTIONS...\n",
    "\n",
    "#FIXMMEEEEEEEE make it so these can be called on arrays where M>1!!!!! BECAUSE THIS SUCKS\n",
    "\n",
    "def global_iou(predictions):\n",
    "    \n",
    "    '''takes the iou of multiple different segmentations'''\n",
    "    \n",
    "    intersection = np.min(predictions,axis=0).sum()\n",
    "    union = np.max(predictions,axis=0).sum()\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def global_dsc(predictions):\n",
    "    \n",
    "    N = predictions.shape[0]\n",
    "    numerator = N * np.min(predictions,axis=0).sum()\n",
    "    denominator = predictions.sum()\n",
    "    \n",
    "    return numerator/denominator\n",
    "    \n",
    "def mean_pairwise_iou(predictions):\n",
    "    \n",
    "    #all combinations of inputs\n",
    "    ious = [iou(a,b) for a,b in itertools.combinations(predictions,2)]\n",
    "    \n",
    "    return np.mean(ious)\n",
    "\n",
    "def mean_pairwise_dsc(predictions):\n",
    "    \n",
    "    #all combinations of samples, which will be axis 0\n",
    "    dscs = [dsc(a,b) for a,b in itertools.combinations(predictions,2)]\n",
    "    \n",
    "    return np.mean(dscs)\n",
    "    \n",
    "def voxel_uncertainty(predictions):\n",
    "    \n",
    "    '''voxel-wise uncertainty as defined in Roy et al (2018)'''\n",
    "    \n",
    "    #strcture-and-voxel-wise uncertainty (compresses over the sample axis\n",
    "    feature_uncertainty = -np.sum(predictions*np.log(predictions),axis = 0)\n",
    "    #global uncertainty is the sum over the feature axis\n",
    "    global_uncertainty = np.sum(feature_uncertainty,axis=-1)\n",
    "    \n",
    "    return global_uncertainty\n",
    "    \n",
    "def mean_std_area(predictions):\n",
    "    \n",
    "    '''the area occupied by each segmented channel. outputs two array: mean and standard deviation\n",
    "    RETURNS ANSWERS IN PIXELS WHICH MUST BE RESCALED LATER!!!!!!\n",
    "    '''\n",
    "    #get the dims\n",
    "    N = predictions.shape[0]\n",
    "    nPixels = np.product(predictions.shape[1:-1])\n",
    "    nFeatures = predictions.shape[-1]\n",
    "    \n",
    "    #reshape array so that it is (N,pixels,features) and thrshold.\n",
    "    predictions = predictions.reshape((N,nPixels,nFeatures)) > 0.5\n",
    "    \n",
    "    #sum of voxels for each \n",
    "    areas = np.sum(predictions,axis = 1)\n",
    "    \n",
    "    #mean, returning a value for each segmentation channel\n",
    "    mu = np.mean(areas,axis=0)\n",
    "    sigma = np.std(areas,axis=0)\n",
    "    \n",
    "    return mu,sigma\n",
    "\n",
    "\n",
    "def predict_stochastic(model,N,X):\n",
    "    \n",
    "    '''draw and summarise multiple predictions from a model\n",
    "    Arguments:\n",
    "        model {a model, for example a Keras model, with a predict method} -- is assumed to have some stochastic component, i.e. multiple\n",
    "        N {int} -- the number of sample predictions to be drawn from the stochastic model\n",
    "        X {numpy array, probably float} -- assumed to be already consistent with inputs to the model\n",
    "        \n",
    "    Returns:\n",
    "        consensus {numpy array, most likely float although can be binarised} -- pixelwise segmentation of x\n",
    "    '''\n",
    "    \n",
    "    #draw N predictions from the model over x\n",
    "    predictions = np.stack([model.predict(X) for n in range(N)],axis=0)\n",
    "    \n",
    "    #binarise\n",
    "    predictions = predictions\n",
    "    \n",
    "    consensus = np.mean(predictions,axis=0)>0.5\n",
    "    \n",
    "    #metrics described in Roy et al...\n",
    "    uncertainty = voxel_uncertainty(predictions)\n",
    "    \n",
    "    mpDsc = mean_pairwise_dsc(predictions)\n",
    "    gDsc = global_dsc(predictions)\n",
    "    \n",
    "    mpIou = mean_pairwise_iou(predictions)\n",
    "    gIou = global_iou(predictions)\n",
    "    meanArea,stdArea = mean_std_area(predictions)\n",
    "    \n",
    "    return consensus,uncertainty,meanArea,stdArea,mpDsc,gDsc,mpIou,gIou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the  distribution of IoU, hausdorff distance and mean contour distance, for each example image in train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 #15 from Roy et al, meh.\n",
    "\n",
    "predTest,uncertaintyTest,meanAreaTest,stdAreaTest,mpDscTest,gDscTest,mpIouTest,gIouTest = map(np.array,zip(*[predict_stochastic(model,N,x.reshape(1,208,208,1)) for x in X_test]))\n",
    "predTrain,uncertaintyTrain,meanAreaTrain,stdAreaTrain,mpDscTrain,gDscTrain,mpIouTrain,gIouTrain = map(np.array,zip(*[predict_stochastic(model,N,x.reshape(1,208,208,1)) for x in X]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTrain = predTrain.reshape(345,208,208,1)\n",
    "predTest = predTest.reshape(87,208,208,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loop over th eexample axis, calculating metrics for each image separately\n",
    "TrainIOU = [iou(Y[m,:,:,:], predTrain[m,:,:]) for m in range(M)]\n",
    "TestIOU = [iou(Y_test[m,:,:,:], predTest[m,:,:]) for m in range(MTest)]\n",
    "\n",
    "TrainDSC = [dsc(Y[m,:,:,:], predTrain[m,:,:]) for m in range(M)]\n",
    "TestDSC = [dsc(Y_test[m,:,:,:], predTest[m,:,:]) for m in range(MTest)]\n",
    "\n",
    "TrainHD = [symmetric_hausdorff_distance(Y[m,:,:,:], predTrain[m,:,:],pxSpacing[m]) for m in range(M)]\n",
    "TestHD = [symmetric_hausdorff_distance(Y_test[m,:,:,:], predTest[m,:,:],pxSpacing_test[m]) for m in range(MTest)]\n",
    "\n",
    "TrainMCD = [mean_contour_distance(Y[m,:,:,:], predTrain[m,:,:],pxSpacing[m]) for m in range(M)]\n",
    "TestMCD = [mean_contour_distance(Y_test[m,:,:,:], predTest[m,:,:],pxSpacing_test[m]) for m in range(MTest)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at network performance.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms for each of the metrics...\n",
    "\n",
    "plt.figure(figsize = (20,5))\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.hist(TrainIOU , bins = np.arange(0,1.05,0.05), density=True, alpha=0.5, label = 'Train')\n",
    "plt.hist(TestIOU ,  bins = np.arange(0,1.05,0.05), density=True, alpha=0.5, label = 'Test')\n",
    "plt.xlabel('Intersection-over-Union')\n",
    "plt.title(f'test mean = {np.mean(TestIOU):.3f}')\n",
    "\n",
    "plt.ylabel('probability density')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.hist(TrainDSC , bins = np.arange(0,1.05,0.05), density=True, alpha=0.5, label = 'Train')\n",
    "plt.hist(TestDSC ,  bins = np.arange(0,1.05,0.05), density=True, alpha=0.5, label = 'Test')\n",
    "plt.xlabel('Dice-Sorenson coefficient')\n",
    "plt.title(f'test mean = {np.mean(TestDSC):.3f}')\n",
    "\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.hist(TrainHD , bins = np.arange(0,125,5), density=True, alpha=0.5, label = 'Train')\n",
    "plt.hist(TestHD , bins = np.arange(0,125,5), density=True, alpha=0.5, label = 'Test')\n",
    "plt.xlabel('Hausdorff Distance (mm)')\n",
    "plt.title(f'test mean = {np.mean(TestHD):.3f}')\n",
    "\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.hist(TrainMCD , bins = np.arange(0,25,2),density=True, alpha=0.5, label = 'Train')\n",
    "plt.hist(TestMCD , bins = np.arange(0,25,2), density=True, alpha=0.5, label = 'Test')\n",
    "plt.xlabel('Mean Contour Distance (mm)')\n",
    "plt.title(f'test mean = {np.mean(TestMCD):.3f}')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    " \n",
    "plt.savefig(outputName + '_metrics_histogram.svg')\n",
    "plt.savefig(outputName + '_metrics_histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, are there correlations between predicted and real metric values????? FUUUUUUUUCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two ground-truths to be predicted\n",
    "\n",
    "#4 potential predictions to be made...\n",
    "plt.figure(figsize =(20,10))\n",
    "\n",
    "metrics = [mpDscTest,gDscTest,mpIouTest,gIouTest]\n",
    "metricNames = ['mean pairwise Dice coefficient',\n",
    "               'global Dice coefficient',\n",
    "               'mean pairwise IOU',\n",
    "               'global IOU'\n",
    "              ]\n",
    "\n",
    "def scatter_with_title(x,y):\n",
    "    plt.plot([0,1],[0,1],c='k')\n",
    "    plt.scatter(x,y)\n",
    "    r,p = pearsonr(x,y)\n",
    "    plt.title(f'r = {r:.2f}')\n",
    "    plt.axis('equal')\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "\n",
    "    \n",
    "for metricInd in range(4):\n",
    "    plt.subplot(2,4,metricInd+1)\n",
    "    scatter_with_title(metrics[metricInd],TestDSC)\n",
    "    if metricInd ==0:\n",
    "        plt.ylabel('True DSC')\n",
    "\n",
    "    plt.subplot(2,4,5+metricInd)\n",
    "    scatter_with_title(metrics[metricInd],TestIOU)\n",
    "    if metricInd ==0:\n",
    "        plt.ylabel('True IOU')\n",
    "    plt.xlabel(metricNames[metricInd])\n",
    "\n",
    "plt.savefig(outputName + '_QC_predictions.svg')\n",
    "plt.savefig(outputName + '_QC_predictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YEEEEEEESSSSSSSSSSSSSSSMOTHAFYUCKEERRRRRRRRRRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a few examples of the training set segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negs = 25\n",
    "\n",
    "egs = np.random.choice(range(M), negs, replace=False)\n",
    "\n",
    "ncols = 5\n",
    "nrows = np.ceil(negs/ncols)\n",
    "\n",
    "plt.figure(figsize = (5*ncols,5*nrows))\n",
    "\n",
    "imShape = X.shape[1:-1]\n",
    "\n",
    "for i in range(negs):\n",
    "    \n",
    "    plt.subplot(nrows,ncols,i+1)\n",
    "    \n",
    "    manual,automated = Y[egs[i],:,:].reshape(imShape), predTrain[egs[i],:,:].reshape(imShape) > 0.5\n",
    "    \n",
    "    pxS = pxSpacing[egs[i]]\n",
    "\n",
    "    \n",
    "    show_image_with_masks(image = X[egs[i],:,:].reshape(imShape),\n",
    "                          masks = [manual,automated],\n",
    "                          maskOptions = [{'linewidth':1,'color':'g'},{'linewidth':1,'color':'r'}]\n",
    "                         )\n",
    "    \n",
    "    plt.title('iou = ' + f'{iou(manual,automated):.03}' + '\\n' + \n",
    "              'hd = ' + f'{symmetric_hausdorff_distance(manual,automated,pxS):.03}' + '\\n' +\n",
    "              'mcd = ' + f'{mean_contour_distance(manual,automated,pxS):.03}')\n",
    "    \n",
    "plt.savefig(outputName + '_train_examples.svg')\n",
    "plt.savefig(outputName + '_train_examples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negs = 25\n",
    "\n",
    "egs = np.random.choice(range(MTest), negs, replace=False)\n",
    "\n",
    "ncols = 5\n",
    "nrows = np.ceil(negs/ncols)\n",
    "\n",
    "plt.figure(figsize = (5*ncols,5*nrows))\n",
    "\n",
    "imShape = X_test.shape[1:-1]\n",
    "\n",
    "for i in range(negs):\n",
    "    \n",
    "    plt.subplot(nrows,ncols,i+1)\n",
    "    \n",
    "    manual,automated = Y_test[egs[i],:,:].reshape(imShape), predTest[egs[i],:,:].reshape(imShape) > 0.5\n",
    "    \n",
    "    pxS = pxSpacing_test[egs[i]]\n",
    "    \n",
    "    show_image_with_masks(image = X_test[egs[i],:,:].reshape(imShape),\n",
    "                          masks = [manual,automated],\n",
    "                          maskOptions = [{'linewidth':1,'color':'g'},{'linewidth':1,'color':'r'}]\n",
    "                         )\n",
    "    \n",
    "    plt.title('iou = ' + f'{iou(manual,automated):.03}' + '\\n' + \n",
    "              'hd = ' + f'{symmetric_hausdorff_distance(manual,automated,pxS):.03}' + '\\n' +\n",
    "              'mcd = ' + f'{mean_contour_distance(manual,automated,pxS):.03}')\n",
    "    \n",
    "    \n",
    "plt.savefig(outputName + '_test_examples.svg')\n",
    "plt.savefig(outputName + '_test_examples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, save the model for use elsewhere, along with some performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#need to save architecture and weight separately as custom loss functions cause issues with loading from a single .h5\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open( outputName + '.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(outputName + '.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write some metrics of model performance for the future...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format a line to add to the csv\n",
    "\n",
    "modelDetails = {'Filename': outputName,\n",
    "                'TrainIOUMean': str(np.mean(TrainIOU)),\n",
    "                'TrainIOUStd': str(np.std(TrainIOU)),\n",
    "                'TestIOUMean': str(np.mean(TestIOU)),\n",
    "                'TestIOUStd': str(np.std(TestIOU)),\n",
    "                'TrainHDMean': str(np.mean(TrainHD)),\n",
    "                'TrainHDStd': str(np.std(TrainHD)),\n",
    "                'TestHDMean': str(np.mean(TestHD)),\n",
    "                'TestHDStd': str(np.std(TestHD)),\n",
    "                'TrainMCDMean': str(np.mean(TrainMCD)),\n",
    "                'TrainMCDStd': str(np.std(TrainMCD)),\n",
    "                'TestMCDMean': str(np.mean(TestMCD)),\n",
    "                'TestMCDStd': str(np.std(TestMCD))\n",
    "               }\n",
    "\n",
    "#if the file containing details of past models does not exist, then create it (with a header row)\n",
    "historyFile = os.path.join(modelDir,'model_history.csv')\n",
    "if not os.path.isfile(historyFile):\n",
    "    \n",
    "    fields = modelDetails.keys()\n",
    "    \n",
    "    with open(historyFile,'w+') as f:\n",
    "        f.write(','.join(fields) + '\\n')\n",
    "        \n",
    "#now write out the line of performance statistics.\n",
    "with open(historyFile,'a') as f:\n",
    "    f.write(','.join(modelDetails.values()) + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0 GPU",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
