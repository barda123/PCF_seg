{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as keras\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from custom_losses import binary_crossentropy_weight_balance, binary_crossentropy_weight_dict, binary_crossentropy_closeness_to_foreground,dice_coef_loss\n",
    "\n",
    "from mask_utils import show_image_with_masks,iou,symmetric_hausdorff_distance,mean_contour_distance\n",
    "\n",
    "from network_utils import gpu_memory_limit,augmentImageSequence\n",
    "\n",
    "from MultiResUNet.MultiResUNet import MultiResUnet\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit how much GPU RAM can be allocated by this notebook... 8GB is 1/3 of available\n",
    "gpu_memory_limit(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory for keeping models, and journaling their performance/graphs\n",
    "modelDir = os.path.join('data','models')\n",
    "if not os.path.isdir(modelDir):\n",
    "    os.mkdir(modelDir)\n",
    "    \n",
    "dateStr = datetime.now().strftime('%Y-%m-%d_%H:%M')\n",
    "outputName = os.path.join(modelDir,'mrunet_' + dateStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataDir = './data/pericardial/wsx_round2/'\n",
    "\n",
    "#load data - these files created by extract_dcm_for_wsx.ipynb\n",
    "X = np.load(os.path.join(DataDir,'X.npy'))\n",
    "Y = np.load(os.path.join(DataDir,'Y.npy')).astype('float')\n",
    "pxArea = np.load(os.path.join(DataDir,'pxSize.npy'))\n",
    "pxSpacing = np.sqrt(pxArea)\n",
    "\n",
    "#ensure the shape is correct arrays saved were rank 3, so this changes to rank 4 (last dimension represents channels)\n",
    "X = X.reshape([*X.shape,1])\n",
    "Y = Y.reshape([*Y.shape,1])\n",
    "\n",
    "#do train/test split!\n",
    "X, X_test, Y, Y_test,pxArea,pxArea_test,pxSpacing,pxSpacing_test = train_test_split(X, Y, pxArea,pxSpacing, test_size=0.2,random_state=101)\n",
    "\n",
    "#\n",
    "M = X.shape[0]\n",
    "MTest = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#properties for data augmentation - that does nothing except randomise the order\n",
    "# dataGenArgs = dict(rotation_range=0,\n",
    "#                    width_shift_range=0,\n",
    "#                    height_shift_range=0,\n",
    "#                    shear_range=0,#0.05,\n",
    "#                    zoom_range=0,\n",
    "#                    horizontal_flip=False,\n",
    "#                    vertical_flip=False,\n",
    "#                    fill_mode='nearest',\n",
    "#                    data_format= 'channels_last',\n",
    "#                    featurewise_center=False,\n",
    "#                    featurewise_std_normalization=False,\n",
    "#                    zca_whitening=False,\n",
    "#                   )\n",
    "\n",
    "# #REAL properties for data augmentation\n",
    "dataGenArgs = dict(rotation_range=10,\n",
    "                   width_shift_range=0.1,\n",
    "                   height_shift_range=0.1,\n",
    "                   shear_range=0.05,\n",
    "                   zoom_range=0.1,\n",
    "                   horizontal_flip=False, #DO NOT FLIP THE IMAGES FFS\n",
    "                   vertical_flip=False,\n",
    "                   fill_mode='nearest',\n",
    "                   data_format= 'channels_last',\n",
    "                   featurewise_center=False,\n",
    "                   featurewise_std_normalization=False,\n",
    "                   zca_whitening=False,\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "earlyStop = callbacks.EarlyStopping(patience=10, #be a bit patient...\n",
    "                                    min_delta=0,\n",
    "                                    monitor='loss',\n",
    "                                    restore_best_weights=True,\n",
    "                                    mode='min',\n",
    "                                   )\n",
    "\n",
    "reduceLR = callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       patience=5,\n",
    "                                       factor=0.3,\n",
    "                                       verbose=1,\n",
    "                                       cooldown=5,\n",
    "                                      )\n",
    "\n",
    "CALLBACKS = [earlyStop,\n",
    "             reduceLR\n",
    "            ]\n",
    "\n",
    "OPT = Adam(learning_rate = 1e-2,\n",
    "           beta_1 = 0.9,\n",
    "           beta_2 = 0.999,\n",
    "           amsgrad = False\n",
    "          )\n",
    "\n",
    "#other hyperparameters\n",
    "BATCHSIZE = 10 #THIS MATTERS A LOT\n",
    "\n",
    "#Spatial smoothing for pixel weights\n",
    "SIGMA = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.clear_session()\n",
    "\n",
    "tf.random.set_seed(101) #FIXME!!! this is not sufficient to guarantee deterministic behaviour during fitting.\n",
    "\n",
    "model = MultiResUnet(height=X.shape[1],width=X.shape[2],n_channels=1)\n",
    "\n",
    "model.compile(optimizer = OPT, \n",
    "              loss = 'binary_crossentropy',\n",
    "#               loss = binary_crossentropy_weight_balance,\n",
    "#               loss = binary_crossentropy_closeness_to_foreground(sigma=SIGMA),\n",
    "#               loss = dice_coef_loss,\n",
    "              metrics = ['accuracy',metrics.MeanIoU(num_classes=2)],\n",
    "             )\n",
    "\n",
    "fitHistory = model.fit(augmentImageSequence(X,Y,dataGenArgs,batchSize=BATCHSIZE),\n",
    "                       epochs = 300,#think about me... \n",
    "                       steps_per_epoch= M//BATCHSIZE, #obvs\n",
    "                       workers=8,\n",
    "                       use_multiprocessing=True,\n",
    "                       validation_data=(X_test,Y_test.astype('float')),\n",
    "                       callbacks=CALLBACKS,\n",
    "                       verbose=1,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at how fitting has proceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(fitHistory.history['loss'],label = 'train')\n",
    "plt.plot(fitHistory.history['val_loss'],label = 'dev')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xticks([])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(fitHistory.history['mean_io_u'],label = 'train')\n",
    "plt.plot(fitHistory.history['val_mean_io_u'],label = 'dev')\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel('mean iou')\n",
    "\n",
    "plt.xlabel('epoch #')\n",
    "\n",
    "plt.savefig(outputName + '_loss_history.svg')\n",
    "plt.savefig(outputName + '_loss_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the values actually output by the model - as there is some instability atm this is pretty important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTest = model.predict(X_test)\n",
    "\n",
    "predTrain = model.predict(X)\n",
    "\n",
    "#show the actual distribution of output values\n",
    "plt.hist(predTrain.flatten(),density=True, alpha=0.5, label = 'Train',bins = np.arange(0,1.05,0.1))\n",
    "plt.hist(predTest.flatten(), density=True, alpha=0.5, label = 'Test' ,bins = np.arange(0,1.05,0.1),)\n",
    "# plt.title('distribution of output values over all pixels')\n",
    "plt.legend()\n",
    "plt.xlabel('value')\n",
    "plt.ylabel('probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the  distribution of IoU, hausdorff distance and mean contour distance, for each example image in train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loop over th eexample axis, calculating metrics for each image separately\n",
    "TrainIOU = [iou(Y[m,:,:,:], predTrain[m,:,:]) for m in range(M)]\n",
    "TestIOU = [iou(Y_test[m,:,:,:], predTest[m,:,:]) for m in range(MTest)]\n",
    "\n",
    "TrainHD = [symmetric_hausdorff_distance(Y[m,:,:,:], predTrain[m,:,:],pxSpacing[m]) for m in range(M)]\n",
    "TestHD = [symmetric_hausdorff_distance(Y_test[m,:,:,:], predTest[m,:,:],pxSpacing_test[m]) for m in range(MTest)]\n",
    "\n",
    "TrainMCD = [mean_contour_distance(Y[m,:,:,:], predTrain[m,:,:],pxSpacing[m]) for m in range(M)]\n",
    "TestMCD = [mean_contour_distance(Y_test[m,:,:,:], predTest[m,:,:],pxSpacing_test[m]) for m in range(MTest)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms for each of the metrics...\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "\n",
    "plt.hist(TrainIOU , bins = np.arange(0,1.05,0.05), density=True, alpha=0.5, label = 'Train')\n",
    "plt.hist(TestIOU ,  bins = np.arange(0,1.05,0.05), density=True, alpha=0.5, label = 'Test')\n",
    "\n",
    "plt.xlabel('Intersection-over-Union')\n",
    "plt.ylabel('probability density')\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "\n",
    "\n",
    "plt.hist(TrainHD , bins = np.arange(0,125,5), density=True, alpha=0.5, label = 'Train')\n",
    "plt.hist(TestHD , bins = np.arange(0,125,5), density=True, alpha=0.5, label = 'Test')\n",
    "\n",
    "plt.xlabel('Hausdorff Distance (mm)')\n",
    "# plt.ylabel('probability density')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "\n",
    "plt.hist(TrainMCD , bins = np.arange(0,25,2),density=True, alpha=0.5, label = 'Train')\n",
    "plt.hist(TestMCD , bins = np.arange(0,25,2), density=True, alpha=0.5, label = 'Test')\n",
    "\n",
    "plt.xlabel('Mean Contour Distance (mm)')\n",
    "# plt.ylabel('probability density')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    " \n",
    "plt.savefig(outputName + '_metrics_histogram.svg')\n",
    "plt.savefig(outputName + '_metrics_histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do predicted **areas** of fat match? That is what the project is all about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "\n",
    "areasPredTrain = np.sum(predTrain,axis=(1,2,3)) * pxArea / 100\n",
    "areasTrueTrain = np.sum(Y,axis=(1,2,3)) * pxArea / 100\n",
    "\n",
    "areasPredTest = np.sum(predTest,axis=(1,2,3)) * pxArea_test / 100\n",
    "areasTrueTest = np.sum(Y_test,axis=(1,2,3)) * pxArea_test / 100\n",
    "\n",
    "plt.scatter(areasTrueTrain,areasPredTrain,label = 'train',alpha=0.6)\n",
    "plt.scatter(areasTrueTest,areasPredTest,label = 'test',alpha=0.6)\n",
    "\n",
    "r,p = pearsonr(areasTrueTest,areasPredTest)\n",
    "\n",
    "plt.title('for test set, R$^2$ = ' + f'{r**2:.03}' + ', p = ' + f'{p:.03}')\n",
    "\n",
    "plt.xlabel('human area (cm$^2$)')\n",
    "\n",
    "plt.ylabel('machine area (cm$^2$)')\n",
    "\n",
    "# plt.axis('equal')\n",
    "lims = [0,np.max(np.hstack((areasPredTrain,areasTrueTrain,areasPredTest,areasTrueTest)))]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(outputName + '_area_correlation.svg')\n",
    "plt.savefig(outputName + '_area_correlation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a few examples of the training set segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negs = 25\n",
    "\n",
    "egs = np.random.choice(range(M), negs, replace=False)\n",
    "\n",
    "ncols = 5\n",
    "nrows = np.ceil(negs/ncols)\n",
    "\n",
    "plt.figure(figsize = (5*ncols,5*nrows))\n",
    "\n",
    "imShape = X.shape[1:-1]\n",
    "\n",
    "for i in range(negs):\n",
    "    \n",
    "    plt.subplot(nrows,ncols,i+1)\n",
    "    \n",
    "    manual,automated = Y[egs[i],:,:].reshape(imShape), predTrain[egs[i],:,:].reshape(imShape) > 0.5\n",
    "    \n",
    "    pxS = pxSpacing[egs[i]]\n",
    "\n",
    "    \n",
    "    show_image_with_masks(image = X[egs[i],:,:].reshape(imShape),\n",
    "                          masks = [manual,automated],\n",
    "                          maskOptions = [{'linewidth':1,'color':'g'},{'linewidth':1,'color':'r'}]\n",
    "                         )\n",
    "    \n",
    "    plt.title('iou = ' + f'{iou(manual,automated):.03}' + '\\n' + \n",
    "              'hd = ' + f'{symmetric_hausdorff_distance(manual,automated,pxS):.03}' + '\\n' +\n",
    "              'mcd = ' + f'{mean_contour_distance(manual,automated,pxS):.03}')\n",
    "    \n",
    "plt.savefig(outputName + '_train_examples.svg')\n",
    "plt.savefig(outputName + '_train_examples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negs = 25\n",
    "\n",
    "egs = np.random.choice(range(MTest), negs, replace=False)\n",
    "\n",
    "ncols = 5\n",
    "nrows = np.ceil(negs/ncols)\n",
    "\n",
    "plt.figure(figsize = (5*ncols,5*nrows))\n",
    "\n",
    "imShape = X_test.shape[1:-1]\n",
    "\n",
    "for i in range(negs):\n",
    "    \n",
    "    plt.subplot(nrows,ncols,i+1)\n",
    "    \n",
    "    manual,automated = Y_test[egs[i],:,:].reshape(imShape), predTest[egs[i],:,:].reshape(imShape) > 0.5\n",
    "    \n",
    "    pxS = pxSpacing_test[egs[i]]\n",
    "    \n",
    "    show_image_with_masks(image = X_test[egs[i],:,:].reshape(imShape),\n",
    "                          masks = [manual,automated],\n",
    "                          maskOptions = [{'linewidth':1,'color':'g'},{'linewidth':1,'color':'r'}]\n",
    "                         )\n",
    "    \n",
    "    plt.title('iou = ' + f'{iou(manual,automated):.03}' + '\\n' + \n",
    "              'hd = ' + f'{symmetric_hausdorff_distance(manual,automated,pxS):.03}' + '\\n' +\n",
    "              'mcd = ' + f'{mean_contour_distance(manual,automated,pxS):.03}')\n",
    "    \n",
    "    \n",
    "plt.savefig(outputName + '_test_examples.svg')\n",
    "plt.savefig(outputName + '_test_examples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, save the model for use elsewhere, along with some performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#need to save architecture and weight separately as custom loss functions cause issues with loading from a single .h5\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open( outputName + '.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(outputName + '.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write some metrics of model performance for the future...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format a line to add to the csv\n",
    "\n",
    "modelDetails = {'Filename': outputName,\n",
    "                'TrainIOUMean': str(np.mean(TrainIOU)),\n",
    "                'TrainIOUStd': str(np.std(TrainIOU)),\n",
    "                'TestIOUMean': str(np.mean(TestIOU)),\n",
    "                'TestIOUStd': str(np.std(TestIOU)),\n",
    "                'TrainHDMean': str(np.mean(TrainHD)),\n",
    "                'TrainHDStd': str(np.std(TrainHD)),\n",
    "                'TestHDMean': str(np.mean(TestHD)),\n",
    "                'TestHDStd': str(np.std(TestHD)),\n",
    "                'TrainMCDMean': str(np.mean(TrainMCD)),\n",
    "                'TrainMCDStd': str(np.std(TrainMCD)),\n",
    "                'TestMCDMean': str(np.mean(TestMCD)),\n",
    "                'TestMCDStd': str(np.std(TestMCD))\n",
    "               }\n",
    "\n",
    "#if the file containing details of past models does not exist, then create it (with a header row)\n",
    "historyFile = os.path.join(modelDir,'model_history.csv')\n",
    "if not os.path.isfile(historyFile):\n",
    "    \n",
    "    fields = modelDetails.keys()\n",
    "    \n",
    "    with open(historyFile,'w+') as f:\n",
    "        f.write(','.join(fields) + '\\n')\n",
    "        \n",
    "#now write out the line of performance statistics.\n",
    "with open(historyFile,'a') as f:\n",
    "    f.write(','.join(modelDetails.values()) + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0 GPU",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
