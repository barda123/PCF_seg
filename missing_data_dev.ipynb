{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for developing the script for syncing data to a local machine, in order to speed up manual segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import glob\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the relevant spreadsheet\n",
    "\n",
    "matchedSubjects = pd.read_csv('./data/matched_diabetics_healthy_all.csv') #created by matching_diabetics_healthy_controls.ipynb\n",
    "\n",
    "#convert to just a list of all of the feids\n",
    "allFeids = pd.concat((matchedSubjects['diabetic feid'].dropna().astype('int'),matchedSubjects['healthy feid'].dropna().astype('int'))).reset_index(drop=True).apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('./data/healthy_diabetics.csv')\n",
    "\n",
    "allFeids = original['f.eid'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rawTopDir = './data/imaging_by_participant/'\n",
    "\n",
    "def get_path_to_participant_raw(topDir,feid , suffix ,search = True):\n",
    "    \n",
    "    #search allows us to chase up files which aren't in the expected location... but is SLOW.\n",
    "    \n",
    "    tryIt = os.path.join(topDir, (feid[:2] + 'xxxxx'), feid, (feid + suffix) )\n",
    "    \n",
    "    if os.path.isfile(tryIt):        \n",
    "        return tryIt\n",
    "    elif search:\n",
    "        print('searching...')\n",
    "        allOptions = glob.glob('./data/**/'+ feid + suffix,recursive=True)\n",
    "        clear_output()\n",
    "        if len(allOptions) >= 1:\n",
    "            return allOptions[0]\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "allLaxFiles = allFeids.apply(lambda x: get_path_to_participant_raw(rawTopDir, str(x) , '_longaxis.zip' , search=False))\n",
    "\n",
    "\n",
    "allScoutFiles = allFeids.apply(lambda x: get_path_to_participant_raw(rawTopDir, str(x) , '_scout.zip' , search=False))\n",
    "\n",
    "print('missing data for ' + str((allLaxFiles=='').sum()) + ' LAX files, and ' + str((allScoutFiles=='').sum()) + ' scout files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_and(allScoutFiles=='',~(allLaxFiles=='')).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having manually checked these, they are not on the GPU machine... Why not? :'(\n",
    "\n",
    "anyway..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulkFile1 = '/workspace/storage/restricted-biobank/releases/REVISION_May2019/ID-27289/ukb27289.bulk'\n",
    "\n",
    "bulkFile2 = '/workspace/storage/restricted-biobank/releases/REVISION_May2019/ID-29801/ukb29801.bulk'\n",
    "\n",
    "\n",
    "#load the bulk files as raw text... easier to search for \n",
    "with open(bulkFile1,'r') as f:\n",
    "    \n",
    "    bulk = f.read()\n",
    "    \n",
    "with open(bulkFile2,'r') as f:\n",
    "    \n",
    "    bulk += f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoutInBulk = allFeids.apply(lambda x: (x + ' 20207') in bulk)\n",
    "\n",
    "laxInBulk = allFeids.apply(lambda x: (x + ' 20208') in bulk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaxInBulkButNoData = np.logical_and(laxInBulk, allLaxFiles=='')\n",
    "ScoutInBulkButNoData = np.logical_and(scoutInBulk, allScoutFiles=='')\n",
    "\n",
    "print(str(LaxInBulkButNoData.sum()) + ' LAX files mentioned in bulk file but not found. ' + str(ScoutInBulkButNoData.sum()) + ' similar for Scout files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r9a1 = pd.read_csv('/images/imaging_by_participant/index_and_data_extract_unprocessed_r9a.csv')\n",
    "\n",
    "r9a2 = pd.read_csv('/images/imaging_by_participant/inventory_heart_MRI_plus_values_r9a.csv')\n",
    "\n",
    "csvFeids = np.unique(np.concatenate((r9a1['f.eid'].values,r9a2['feid'].values)) ).astype(str)\n",
    "\n",
    "in_r9acsv = allFeids.apply(lambda x: x in csvFeids)\n",
    "\n",
    "print ( 'there are ' + str((~in_r9acsv).sum()) + ' feids not mentioned in the r9a_csv files')\n",
    "\n",
    "print('of these, ' + str(np.logical_and(~in_r9acsv,allScoutFiles=='').sum()) + ' do not have data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overlap between missing from bulk and missing from csv\n",
    "\n",
    "print('there are ' +\n",
    "      str(np.logical_and(~in_r9acsv,laxInBulk).sum()) + \n",
    "      ' feids that do have lax listed in the bulk but are not in the csvs, and ' + \n",
    "      str(np.logical_and(~in_r9acsv,scoutInBulk).sum()) + \n",
    "      ' feids that have scouts listed in the bulk but are missing from the csvs '\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data missing but ARE listed in csvs\n",
    "\n",
    "missingLaxButListedInCsv = np.logical_and(allLaxFiles=='',in_r9acsv)\n",
    "\n",
    "feids_missingLaxButListedInCsv = allFeids[missingLaxButListedInCsv]\n",
    "\n",
    "r9a2.set_index(r9a2['feid'].astype(str)).loc[feids_missingLaxButListedInCsv,'Long_axis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data missing but ARE listed in csvs\n",
    "\n",
    "missingScoutButListedInCsv = np.logical_and(allScoutFiles=='',in_r9acsv)\n",
    "\n",
    "feids_missingScoutButListedInCsv = allFeids[missingScoutButListedInCsv]\n",
    "\n",
    "r9a2.set_index(r9a2['feid'].astype(str)).loc[feids_missingScoutButListedInCsv,'Scout_images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "turn back into a single dataframe, and write out so it can be circulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({'feid':allFeids,\n",
    "                        'listed in inventory_heart_MRI_plus_values_r9a.csv':in_r9acsv,\n",
    "                        'scout zipfile found':(allScoutFiles!=''),\n",
    "                        'scout listed in .bulk files':scoutInBulk,\n",
    "                        'lax zipfile found':(allLaxFiles!=''),\n",
    "                        'lax listed in .bulk files':laxInBulk,\n",
    "                       })\n",
    "\n",
    "laxMissing = summary.loc[allLaxFiles=='',:]\n",
    "\n",
    "laxMissing.to_csv('./data/summary_lax_missing_records.csv',index=False)\n",
    "\n",
    "summary.to_csv('./data/summary_records.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we should also just MAKE a new .bulk file, which can be used to make a new query against the UKBB system. This should include both scout and lax images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingLax = allLaxFiles==''\n",
    "missingScout = allScoutFiles==''\n",
    "\n",
    "\n",
    "#FIRST: THE FILES WHICH ARE LISTED IN BULK FILES ALREADY BUT NOT ON OUR SYSTEM\n",
    "#add the identifier code to each feid, and convert to a string with a newline for each one.\n",
    "BulkMissingLax = '\\n'.join(allFeids[LaxInBulkButNoData].apply(lambda x: x+' 20208_2_0').values)\n",
    "BulkMissingScout = '\\n'.join(allFeids[ScoutInBulkButNoData].apply(lambda x: x+' 20207_2_0').values)\n",
    "\n",
    "bulkQuery = BulkMissingLax + '\\n' + BulkMissingScout\n",
    "\n",
    "#write it out\n",
    "with open('./data/existsNotDownloaded.bulk','w+') as f:\n",
    "    f.write(bulkQuery)\n",
    "    \n",
    "#SECOND: THE FILES WHICH ARE SUPPOSED TO EXIST BUT ARE NOT DOWNLOADABLE\n",
    "BulkMissingLax = '\\n'.join(allFeids[np.logical_and(missingLax,~laxInBulk)].apply(lambda x: x+' 20208_2_0').values)\n",
    "BulkMissingScout = '\\n'.join(allFeids[np.logical_and(missingScout,~scoutInBulk)].apply(lambda x: x+' 20207_2_0').values)\n",
    "\n",
    "bulkQuery = BulkMissingLax + '\\n' + BulkMissingScout\n",
    "#write it out\n",
    "with open('./data/shouldExistNotDownloadable.bulk','w+') as f:\n",
    "    f.write(bulkQuery)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is there anything in particular about the dates on which people were imaged??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance = original.loc[:,['f.eid','bio.Date.of.attending.assessment.centre.0.imaging']].set_index(original['f.eid'].astype(str))\n",
    "\n",
    "attendance = attendance['bio.Date.of.attending.assessment.centre.0.imaging'].apply(lambda x: datetime.datetime.strptime(x,'%d/%m/%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "startyear = 2015\n",
    "startmonth = 6 \n",
    "\n",
    "endyear = 2019\n",
    "endmonth = 4\n",
    "\n",
    "months = np.array([np.datetime64(datetime.datetime(m//12, m%12+1, 1),'ns') for m in range(startyear*12+startmonth-1, endyear*12+endmonth)])\n",
    "\n",
    "plt.hist(attendance[allFeids[~(allLaxFiles=='')]].values,alpha = 0.5,bins=months,label = 'found LAX files')\n",
    "\n",
    "plt.hist(attendance[allFeids[allLaxFiles=='']].values,alpha = 0.5,bins=months,label = 'missing LAX files')\n",
    "\n",
    "plt.xlabel('date of imaging')\n",
    "\n",
    "plt.ylabel('n')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
