{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Bayesian framework of Roy et al (2018) in combination with the MultiResUNet architecture, but performs a grid search over the hyperparameters specific to this framework:\n",
    " - r, the dropout probability\n",
    " - the position of the dropout layers (either for every convolutional layer, or at the end of every residual block)\n",
    " - N, the number of samples drawn from the stochastic network for each prediction/QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as keras#\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# from custom_losses import binary_crossentropy_weight_balance, binary_crossentropy_weight_dict, binary_crossentropy_closeness_to_foreground,dice_coef_loss\n",
    "\n",
    "from mask_utils import show_image_with_masks,iou,symmetric_hausdorff_distance,mean_contour_distance,dsc\n",
    "\n",
    "from network_utils import gpu_memory_limit,augmentImageSequence\n",
    "\n",
    "from MultiResUNet.MultiResUNet import MultiResUnet\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit how much GPU RAM can be allocated by this notebook... 8GB is 1/3 of available\n",
    "gpu_memory_limit(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataDir = './data/pericardial/wsx_round2/'\n",
    "\n",
    "#load data - these files created by extract_dcm_for_wsx.ipynb\n",
    "X = np.load(os.path.join(DataDir,'X.npy'))\n",
    "Y = np.load(os.path.join(DataDir,'Y.npy')).astype('float')\n",
    "pxArea = np.load(os.path.join(DataDir,'pxSize.npy'))\n",
    "pxSpacing = np.sqrt(pxArea)\n",
    "\n",
    "#ensure the shape is correct arrays saved were rank 3, so this changes to rank 4 (last dimension represents channels)\n",
    "X = X.reshape([*X.shape,1])\n",
    "Y = Y.reshape([*Y.shape,1])\n",
    "\n",
    "#do train/test split!\n",
    "X, X_test, Y, Y_test,pxArea,pxArea_test,pxSpacing,pxSpacing_test = train_test_split(X, Y, pxArea,pxSpacing, test_size=0.2,random_state=101)\n",
    "\n",
    "#\n",
    "M = X.shape[0]\n",
    "MTest = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#properties for data augmentation - that does nothing except randomise the order\n",
    "# dataGenArgs = dict(rotation_range=0,\n",
    "#                    width_shift_range=0,\n",
    "#                    height_shift_range=0,\n",
    "#                    shear_range=0,#0.05,\n",
    "#                    zoom_range=0,\n",
    "#                    horizontal_flip=False,\n",
    "#                    vertical_flip=False,\n",
    "#                    fill_mode='nearest',\n",
    "#                    data_format= 'channels_last',\n",
    "#                    featurewise_center=False,\n",
    "#                    featurewise_std_normalization=False,\n",
    "#                    zca_whitening=False,\n",
    "#                   )\n",
    "\n",
    "# #REAL properties for data augmentation\n",
    "dataGenArgs = dict(rotation_range=10,\n",
    "                   width_shift_range=0.1,\n",
    "                   height_shift_range=0.1,\n",
    "                   shear_range=0.05,\n",
    "                   zoom_range=0.1,\n",
    "                   horizontal_flip=False, #DO NOT FLIP THE IMAGES FFS\n",
    "                   vertical_flip=False,\n",
    "                   fill_mode='nearest',\n",
    "                   data_format= 'channels_last',\n",
    "                   featurewise_center=False,\n",
    "                   featurewise_std_normalization=False,\n",
    "                   zca_whitening=False,\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "earlyStop = callbacks.EarlyStopping(patience=10, #be a bit patient...\n",
    "                                    min_delta=0,\n",
    "                                    monitor='loss',\n",
    "                                    restore_best_weights=True,\n",
    "                                    mode='min',\n",
    "                                   )\n",
    "\n",
    "reduceLR = callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       patience=5,\n",
    "                                       factor=0.3,\n",
    "                                       verbose=1,\n",
    "                                       cooldown=5,\n",
    "                                      )\n",
    "\n",
    "CALLBACKS = [earlyStop,\n",
    "             reduceLR\n",
    "            ]\n",
    "\n",
    "OPT = Adam(learning_rate = 1e-2,\n",
    "           beta_1 = 0.9,\n",
    "           beta_2 = 0.999,\n",
    "           amsgrad = False\n",
    "          )\n",
    "\n",
    "#other hyperparameters\n",
    "BATCHSIZE = 8 #THIS MATTERS A LOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function which takes only arguments regarding the stochastic network components as inputs, and returns a fitted model object \n",
    "def train_model(dropout_rate,dropout_position):\n",
    "        \n",
    "    if dropout_position == 'layer':\n",
    "        layer_dropout_rate = dropout_rate\n",
    "        block_dropout_rate = None\n",
    "    elif dropout_position == 'block':\n",
    "        layer_dropout_rate = None\n",
    "        block_dropout_rate = dropout_rate\n",
    "\n",
    "    keras.clear_session()\n",
    "\n",
    "    tf.random.set_seed(101) #FIXME!!! this is not sufficient to guarantee deterministic behaviour during fitting.\n",
    "\n",
    "    model = MultiResUnet(height=X.shape[1],\n",
    "                         width=X.shape[2],\n",
    "                         n_channels=1,\n",
    "                         layer_dropout_rate=layer_dropout_rate,\n",
    "                         block_dropout_rate=block_dropout_rate\n",
    "                        )\n",
    "\n",
    "    model.compile(optimizer = OPT, \n",
    "                  loss = 'binary_crossentropy',\n",
    "    #               loss = binary_crossentropy_weight_balance,\n",
    "    #               loss = binary_crossentropy_closeness_to_foreground(sigma=SIGMA),\n",
    "#                   loss = dice_coef_loss,\n",
    "#                   metrics = ['accuracy',metrics.MeanIoU(num_classes=2)],\n",
    "                  metrics = ['accuracy']\n",
    "                 )\n",
    "\n",
    "    fitHistory = model.fit(augmentImageSequence(X,Y,dataGenArgs,batchSize=BATCHSIZE),\n",
    "                           epochs = 300, #normally training stops at like 50/60 epochs, so it is very unlikely this will ever be used\n",
    "                           steps_per_epoch= M//BATCHSIZE, #obvs\n",
    "                           workers=2,\n",
    "                           use_multiprocessing=True,\n",
    "                           validation_data=(X_test,Y_test.astype('float')),\n",
    "                           callbacks=CALLBACKS,\n",
    "                           verbose=0,\n",
    "                          )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS FOR DOING STOCHASTIC PREDICTIONS...\n",
    "\n",
    "#FIXMMEEEEEEEE make it so these can be called on arrays where M>1!!!!! BECAUSE THIS SUCKS\n",
    "\n",
    "def global_iou(predictions):\n",
    "    \n",
    "    '''takes the iou of multiple different segmentations'''\n",
    "    \n",
    "    intersection = np.min(predictions,axis=0).sum()\n",
    "    union = np.max(predictions,axis=0).sum()\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def global_dsc(predictions):\n",
    "    \n",
    "    N = predictions.shape[0]\n",
    "    numerator = N * np.min(predictions,axis=0).sum()\n",
    "    denominator = predictions.sum()\n",
    "    \n",
    "    return numerator/denominator\n",
    "    \n",
    "def mean_pairwise_iou(predictions):\n",
    "    \n",
    "    #all combinations of inputs\n",
    "    ious = [iou(a,b) for a,b in itertools.combinations(predictions,2)]\n",
    "    \n",
    "    return np.mean(ious)\n",
    "\n",
    "def mean_pairwise_dsc(predictions):\n",
    "    \n",
    "    #all combinations of samples, which will be axis 0\n",
    "    dscs = [dsc(a,b) for a,b in itertools.combinations(predictions,2)]\n",
    "    \n",
    "    return np.mean(dscs)\n",
    "    \n",
    "def voxel_uncertainty(predictions):\n",
    "    \n",
    "    '''voxel-wise uncertainty as defined in Roy et al (2018)'''\n",
    "    \n",
    "    #strcture-and-voxel-wise uncertainty (compresses over the sample axis\n",
    "    feature_uncertainty = -np.sum(predictions*np.log(predictions),axis = 0)\n",
    "    #global uncertainty is the sum over the feature axis\n",
    "    global_uncertainty = np.sum(feature_uncertainty,axis=-1)\n",
    "    \n",
    "    return global_uncertainty\n",
    "    \n",
    "def mean_std_area(predictions):\n",
    "    \n",
    "    '''the area occupied by each segmented channel. outputs two array: mean and standard deviation\n",
    "    RETURNS ANSWERS IN PIXELS WHICH MUST BE RESCALED LATER!!!!!!\n",
    "    '''\n",
    "    #get the dims\n",
    "    N = predictions.shape[0]\n",
    "    nPixels = np.product(predictions.shape[1:-1])\n",
    "    nFeatures = predictions.shape[-1]\n",
    "    \n",
    "    #reshape array so that it is (N,pixels,features) and thrshold.\n",
    "    predictions = predictions.reshape((N,nPixels,nFeatures)) > 0.5\n",
    "    \n",
    "    #sum of voxels for each \n",
    "    areas = np.sum(predictions,axis = 1)\n",
    "    \n",
    "    #mean, returning a value for each segmentation channel\n",
    "    mu = np.mean(areas,axis=0)\n",
    "    sigma = np.std(areas,axis=0)\n",
    "    \n",
    "    return mu,sigma\n",
    "\n",
    "def predict_stochastic(model,N,X):\n",
    "    \n",
    "    '''draw and summarise multiple predictions from a model\n",
    "    Arguments:\n",
    "        model {a model, for example a Keras model, with a predict method} -- is assumed to have some stochastic component, i.e. multiple\n",
    "        N {int} -- the number of sample predictions to be drawn from the stochastic model\n",
    "        X {numpy array, probably float} -- assumed to be already consistent with inputs to the model. MUST ONLY BE A SINGLE IMAGE AND NOT MULTIPLE STACKED>\n",
    "        \n",
    "    Returns:\n",
    "        consensus {numpy array, boolean} -- pixelwise segmentation of x\n",
    "        also various floats, representing different metrics for uncertainty and the outputs.\n",
    "    '''\n",
    "    \n",
    "    #draw N predictions from the model over x\n",
    "    predictions = np.stack([model.predict(X) for n in range(N)],axis=0)\n",
    "    \n",
    "    predictions = predictions\n",
    "\n",
    "    #binarise the summary\n",
    "    consensus = np.mean(predictions,axis=0)>0.5 \n",
    "    \n",
    "    #metrics described in Roy et al...\n",
    "    uncertainty = voxel_uncertainty(predictions)\n",
    "    \n",
    "    mpDsc = mean_pairwise_dsc(predictions)\n",
    "    gDsc = global_dsc(predictions)\n",
    "    \n",
    "    mpIou = mean_pairwise_iou(predictions)\n",
    "    gIou = global_iou(predictions)\n",
    "    meanArea,stdArea = mean_std_area(predictions)\n",
    "    \n",
    "    return consensus,uncertainty,meanArea,stdArea,mpDsc,gDsc,mpIou,gIou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_metrics(model,N):\n",
    "    \n",
    "    '''takes a trained model as input, and returns summary metrics for the test set'''\n",
    "    \n",
    "    predTest,uncertainty,meanArea,stdArea,mpDsc,gDsc,mpIou,gIou = map(np.array,zip(*[predict_stochastic(model,N,x.reshape(1,208,208,1)) for x in X_test]))\n",
    "    predTest = predTest.reshape(87,208,208,1) #hack\n",
    "    \n",
    "    #loop over th eexample axis, calculating metrics for each image separately\n",
    "    TestIOU = [iou(Y_test[m,:,:,:], predTest[m,:,:,:]) for m in range(MTest)]\n",
    "    TestDSC = [dsc(Y_test[m,:,:,:], predTest[m,:,:,:]) for m in range(MTest)]\n",
    "    \n",
    "    #\n",
    "    \n",
    "    #needs to return MODEL PERFORMANCE - actual metrics not required for this...\n",
    "    \n",
    "    return np.mean(TestIOU),np.mean(TestDSC),TestIOU,TestDSC,mpDsc,gDsc,mpIou,gIou\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resNames = ['dropoutPosition',\n",
    "            'r',\n",
    "            'N',\n",
    "            'true mean IOU',\n",
    "            'true mean DSC',\n",
    "            'true IOU',\n",
    "            'true DSC',\n",
    "            'mean pairwise DSC',\n",
    "            'global DSC',\n",
    "            'mean pairwise IOU',\n",
    "            'global IOU'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all hyperparameter combinations\n",
    "dropoutPositions = ['layer','block']\n",
    "dropoutRates = [0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.4,0.5]\n",
    "Ns = [5,10,15,20,25,30,50,100]\n",
    "#all combinations - this ORDER is good as we want trained models to be reused as far as possible...\n",
    "dropoutPositions,dropoutRates,Ns = [x.flatten() for x in np.meshgrid(dropoutPositions,dropoutRates,Ns)]\n",
    "\n",
    "\n",
    "#set \"previous\" vars to values not used.\n",
    "previousPos = '808'\n",
    "previousR = 909\n",
    "\n",
    "inputParams = list(zip(dropoutPositions,dropoutRates,Ns))\n",
    "\n",
    "DATAFILE = './data/Bayesian_hyperparameter_tuning.pickle'\n",
    "#initialise results list - check if it's been done already and trim list of parameters to be executed if it has\n",
    "if os.path.isfile(DATAFILE):\n",
    "    results = pickle.load(open(DATAFILE,'rb'))\n",
    "    inputParams = inputParams[len(results):]\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "#loop over\n",
    "for ind,params in enumerate(inputParams):\n",
    "    clear_output()\n",
    "    print('/'.join((str(ind+1),str(len(inputParams)))))\n",
    "    \n",
    "    pos,r,N = params\n",
    "    \n",
    "    if previousPos != pos and previousR != r: #i.e. if we need to train a new model\n",
    "        model = train_model(r,pos)\n",
    "    \n",
    "    #overwrite previous values\n",
    "    previousPos = pos\n",
    "    previousR = r\n",
    "    \n",
    "    #now use the current value of N to sample from the model and record how well it does, add to params for a dict output\n",
    "    res = params + summary_metrics(model,N)\n",
    "    \n",
    "    #create a dict for this iteration\n",
    "    resultDict = dict(zip(resNames,res))\n",
    "    \n",
    "    #add to list and write a pickle out..\n",
    "    results.append(resultDict)\n",
    "    \n",
    "    pickle.dump(results,open(DATAFILE,'wb+'))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0 GPU",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
