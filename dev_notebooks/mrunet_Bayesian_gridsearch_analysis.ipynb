{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = './data/Bayesian_hyperparameter_tuning.pickle'\n",
    "\n",
    "if os.path.isfile(DATAFILE):\n",
    "    results = pickle.load(open(DATAFILE,'rb'))\n",
    "    print('data to be analysed from ' + str(len(results)) + ' hyperparameter combinations\\n')\n",
    "else:\n",
    "    print('come back another day ! no results found')\n",
    "\n",
    "#convert to dataframe for easier everything.\n",
    "results = pd.DataFrame(results)\n",
    "\n",
    "print('\\n'.join(['results dataframe contains the following columns:'] + list(results.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of results represents a model and its performance. However, before even considering choices of hyperparameters (dropout position, r and N), we need to look at which METRICS we should be selecting on. That is, which of the mean pairwise/global IOU/DSC predicts TRUE IOU/DSC best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = ['true IOU','true DSC']\n",
    "predictions = ['mean pairwise DSC','global DSC','mean pairwise IOU','global IOU']\n",
    "\n",
    "def clean_r2(predictor,result):\n",
    "    \n",
    "    'returns r2 from 2 arrays, as well as replacing nans with 0s to make consistent'\n",
    "    \n",
    "    predictor[np.isnan(predictor)] = 0\n",
    "    result[np.isnan(result)]=0\n",
    "    #return the actual r2 value\n",
    "    return pearsonr(predictor,result)[0]**2\n",
    "    \n",
    "def get_r2(trueName,predictionName):\n",
    "    \n",
    "    tr = results.loc[:,[trueName,predictionName]]\n",
    "    \n",
    "    #FIXME - when something better shows up, remove np.array conversion (should already be done)\n",
    "    r2s = tr.apply(lambda x: clean_r2(np.array(x[trueName]),np.array(x[predictionName])) ,axis=1)\n",
    "#     r2s = results.apply(lambda x:print(x[trueName],axis=1))\n",
    "    return r2s\n",
    "\n",
    "def mae(predictor,result):\n",
    "    \n",
    "    '''returns the mean absolute error between a set of predictions and their true values'''\n",
    "    \n",
    "    predictor[np.isnan(predictor)] = 0\n",
    "    result[np.isnan(result)]=0\n",
    "    \n",
    "    ae = np.abs(result-predictor)\n",
    "    return np.mean(ae)\n",
    "\n",
    "def get_mae(trueName,predictionName):\n",
    "    \n",
    "    tr = results.loc[:,[trueName,predictionName]]\n",
    "    \n",
    "    #FIXME - when something better shows up, remove np.array conversion (should already be done)\n",
    "    maes = tr.apply(lambda x: mae(np.array(x[trueName]),np.array(x[predictionName])) ,axis=1)\n",
    "\n",
    "    return maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (24,12))\n",
    "\n",
    "bins = np.arange(0,1,0.05)\n",
    "\n",
    "for ind,combination in enumerate(itertools.product(trues,predictions)):\n",
    "    \n",
    "    #first column, calculate and show\n",
    "    r2s = get_r2(*combination)\n",
    "    plt.subplot(4,8,1+ind)\n",
    "    plt.hist(r2s,bins=bins,orientation='horizontal')\n",
    "    plt.ylim([0,1])\n",
    "    plt.title('/'.join(combination))\n",
    "    if ind==0:\n",
    "        plt.ylabel('R^2')\n",
    "    \n",
    "    \n",
    "    maes = get_mae(*combination)\n",
    "    plt.subplot(4,8,9+ind)\n",
    "    plt.hist(maes,bins=bins,orientation='horizontal')\n",
    "    plt.ylim([0,1])\n",
    "    if ind ==0:\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "    plt.subplot(4,8,17+ind)\n",
    "    plt.scatter(maes,r2s)\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlim([0,0.7])\n",
    "    \n",
    "    plt.subplot(4,8,25+ind)\n",
    "    plt.hist((1-maes)*r2s,bins=bins,orientation='horizontal')\n",
    "    plt.ylim([0,1])\n",
    "    if ind ==0:\n",
    "        plt.ylabel('(1-MAE)*R^2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like the best agreement could be provided by the treu DSC/mean pairwise IOU.\n",
    "\n",
    "Interestingly, there are clusters of points where the MAE is low, but so is the R^2. This MUST correspond to a low spread in the test set accuracy (leading to poor linear relationship). Thus, MAE is the more appropriate metric to consider. \n",
    "\n",
    "So, in selecting hyperparameters, we are interested in optimising two things simultaneously:\n",
    " - *maximising* a single actual performance metric (true DSC or true IOU)\n",
    " - *minimising* the MAE between the true DSC and mean pairwise IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes = get_mae('mean pairwise IOU','true DSC')\n",
    "results.loc[:,'mae'] = maes\n",
    "results.loc[:,'r2'] = get_r2('mean pairwise IOU','true DSC')\n",
    "performance = results['true IOU'].apply(np.mean)\n",
    "\n",
    "plt.figure(figsize =(10,10))\n",
    "\n",
    "plt.scatter(maes,performance)\n",
    "\n",
    "plt.xlabel('MAE of accuracy prediction')\n",
    "plt.ylabel('true performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there is a clear right answer - where there is a clear maximum of DSC and minimum of MAE. Now, should do the same but colour according to hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "\n",
    "for p in results['r'].unique():\n",
    "    sel = results['r']==p\n",
    "    plt.scatter(maes[sel],performance[sel],label=p,alpha = 0.4)\n",
    "\n",
    "plt.xlabel('MAE of accuracy prediction')\n",
    "plt.ylabel('true performance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "\n",
    "for p in results['N'].unique():\n",
    "    sel = results['N']==p\n",
    "    plt.scatter(maes[sel],performance[sel],label=p,alpha = 0.4)\n",
    "\n",
    "plt.xlabel('MAE of accuracy prediction')\n",
    "plt.ylabel('true performance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,10))\n",
    "\n",
    "for p in results['dropoutPosition'].unique():\n",
    "    sel = results['dropoutPosition']==p\n",
    "    plt.scatter(maes[sel],performance[sel],label=p,alpha = 0.4)\n",
    "\n",
    "plt.xlabel('MAE of accuracy prediction')\n",
    "plt.ylabel('true performance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that's pretty fucking unambiguous. The best dropout rate is 0.15, placed at the end of every residual block, and the sample size doesn't really make a difference after 20.\n",
    "\n",
    "lets have a look at the scatter plots for the successful examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodModels = results.loc[np.logical_and(maes <0.1,performance > 0.6),:].reset_index()\n",
    "\n",
    "nSuccesses = goodModels.shape[0]\n",
    "\n",
    "ncols = 3\n",
    "\n",
    "nrows = np.ceil(nSuccesses/ncols)\n",
    "\n",
    "plt.figure(figsize = (5*ncols,5*nrows))\n",
    "\n",
    "\n",
    "for ind,row in goodModels.iterrows():\n",
    "\n",
    "    \n",
    "    plt.subplot(nrows,ncols,ind+1)\n",
    "\n",
    "    title = '\\n'.join( ('N = ' + str(row['N']),'mae = ' + str(row['mae']), 'r^2 = ' + str(row['r2']) ) ) \n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.plot([0,1],[0,1],label = 'line of unity',c= 'k')\n",
    "\n",
    "    plt.scatter(row['mean pairwise IOU'],row['true DSC'])\n",
    "\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    \n",
    "plt.legend()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0 GPU",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
