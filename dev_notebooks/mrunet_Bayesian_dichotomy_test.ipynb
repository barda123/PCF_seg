{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for exploring ideas of using different levels of dropout for making predictions and testing whether they are good or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model,model_from_json\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from mask_utils import show_image_with_masks,iou,symmetric_hausdorff_distance,mean_contour_distance,dsc\n",
    "\n",
    "from network_utils import gpu_memory_limit,augmentImageSequence\n",
    "\n",
    "from MultiResUNet.MultiResUNet import MultiResUnet\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit how much GPU RAM can be allocated by this notebook... 8GB is 1/3 of available\n",
    "gpu_memory_limit(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph outputs...\n",
    "DataDir = './data/pericardial/wsx_round2/'\n",
    "\n",
    "splitDataFile = os.path.join(DataDir,'splitData.pickle')\n",
    "\n",
    "if os.path.isfile(splitDataFile):\n",
    "    splitData = pickle.load(open(splitDataFile,'rb'))\n",
    "    X, X_test, Y, Y_test,pxArea,pxArea_test,pxSpacing,pxSpacing_test = splitData\n",
    "    \n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    #load data - these files created by extract_dcm_for_wsx.ipynb\n",
    "    X = np.load(os.path.join(DataDir,'X.npy'))\n",
    "    Y = np.load(os.path.join(DataDir,'Y.npy')).astype('float')\n",
    "    pxArea = np.load(os.path.join(DataDir,'pxSize.npy'))\n",
    "    pxSpacing = np.sqrt(pxArea)\n",
    "\n",
    "    #ensure the shape is correct arrays saved were rank 3, so this changes to rank 4 (last dimension represents channels)\n",
    "    X = X.reshape([*X.shape,1])\n",
    "    Y = Y.reshape([*Y.shape,1])\n",
    "\n",
    "    #do train/test split!\n",
    "    splitData = train_test_split(X, Y, pxArea,pxSpacing, test_size=0.2,random_state=101)\n",
    "    pickle.dump(splitData,open(splitDataFile,'wb'))\n",
    "    #extract individual bits\n",
    "    X, X_test, Y, Y_test,pxArea,pxArea_test,pxSpacing,pxSpacing_test = splitData\n",
    "\n",
    "del splitData #as this variable is o longer required   \n",
    "\n",
    "\n",
    "M = X.shape[0]\n",
    "MTest = X_test.shape[0]\n",
    "imShape = (1,*X.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "modelBaseName = 'mrunet_bayesian_2020-06-15_17:46' \n",
    "\n",
    "modelBaseName = os.path.join('data','models',modelBaseName)\n",
    "\n",
    "modelParamFile = modelBaseName + '.h5'\n",
    "\n",
    "#make sure you know this!!!!!!!!!\n",
    "originalDropoutRate = 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS FOR DOING STOCHASTIC PREDICTIONS...\n",
    "\n",
    "#FIXMMEEEEEEEE make it so these can be called on arrays where M>1!!!!! BECAUSE THIS SUCKS\n",
    "\n",
    "def global_iou(predictions):\n",
    "    \n",
    "    '''takes the iou of multiple different segmentations'''\n",
    "    \n",
    "    intersection = np.min(predictions,axis=0).sum()\n",
    "    union = np.max(predictions,axis=0).sum()\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def global_dsc(predictions):\n",
    "    \n",
    "    N = predictions.shape[0]\n",
    "    numerator = N * np.min(predictions,axis=0).sum()\n",
    "    denominator = predictions.sum()\n",
    "    \n",
    "    return numerator/denominator\n",
    "    \n",
    "def mean_pairwise_iou(predictions):\n",
    "    \n",
    "    #all combinations of inputs\n",
    "    ious = [iou(a,b) for a,b in itertools.combinations(predictions,2)]\n",
    "    \n",
    "    return np.mean(ious)\n",
    "\n",
    "def mean_pairwise_dsc(predictions):\n",
    "    \n",
    "    #all combinations of samples, which will be axis 0\n",
    "    dscs = [dsc(a,b) for a,b in itertools.combinations(predictions,2)]\n",
    "    \n",
    "    return np.mean(dscs)\n",
    "    \n",
    "def voxel_uncertainty(predictions):\n",
    "    \n",
    "    '''voxel-wise uncertainty as defined in Roy et al (2018)'''\n",
    "    \n",
    "    #strcture-and-voxel-wise uncertainty (compresses over the sample axis\n",
    "    feature_uncertainty = -np.sum(predictions*np.log(predictions),axis = 0)\n",
    "    #global uncertainty is the sum over the feature axis\n",
    "    global_uncertainty = np.sum(feature_uncertainty,axis=-1)\n",
    "    \n",
    "    return global_uncertainty\n",
    "    \n",
    "def mean_std_area(predictions):\n",
    "    \n",
    "    '''the area occupied by each segmented channel. outputs two array: mean and standard deviation\n",
    "    RETURNS ANSWERS IN PIXELS WHICH MUST BE RESCALED LATER!!!!!!\n",
    "    '''\n",
    "    #get the dims\n",
    "    N = predictions.shape[0]\n",
    "    nPixels = np.product(predictions.shape[1:-1])\n",
    "    nFeatures = predictions.shape[-1]\n",
    "    \n",
    "    #reshape array so that it is (N,pixels,features) and thrshold.\n",
    "    predictions = predictions.reshape((N,nPixels,nFeatures)) > 0.5\n",
    "    \n",
    "    #sum of voxels for each \n",
    "    areas = np.sum(predictions,axis = 1)\n",
    "    \n",
    "    #mean, returning a value for each segmentation channel\n",
    "    mu = np.mean(areas,axis=0)\n",
    "    sigma = np.std(areas,axis=0)\n",
    "    \n",
    "    return mu,sigma\n",
    "\n",
    "def predict_stochastic(model,N,X):\n",
    "    \n",
    "    '''draw and summarise multiple predictions from a model\n",
    "    Arguments:\n",
    "        model {a model, for example a Keras model, with a predict method} -- is assumed to have some stochastic component, i.e. multiple\n",
    "        N {int} -- the number of sample predictions to be drawn from the stochastic model\n",
    "        X {numpy array, probably float} -- assumed to be already consistent with inputs to the model. MUST ONLY BE A SINGLE IMAGE AND NOT MULTIPLE STACKED!!!!!\n",
    "        \n",
    "    Returns:\n",
    "        consensus {numpy array, boolean} -- pixelwise segmentation of x\n",
    "        also various floats, representing different metrics for uncertainty and the outputs.\n",
    "    '''\n",
    "    \n",
    "    #draw N predictions from the model over x\n",
    "    predictions = np.stack([model.predict(X) for n in range(N)],axis=0)\n",
    "    \n",
    "    #binarise\n",
    "    predictions = predictions\n",
    "    \n",
    "    consensus = np.mean(predictions,axis=0)>0.5 \n",
    "    \n",
    "    #metrics described in Roy et al...\n",
    "    uncertainty = voxel_uncertainty(predictions)\n",
    "    \n",
    "    mpDsc = mean_pairwise_dsc(predictions)\n",
    "    gDsc = global_dsc(predictions)\n",
    "    \n",
    "    mpIou = mean_pairwise_iou(predictions)\n",
    "    gIou = global_iou(predictions)\n",
    "    meanArea,stdArea = mean_std_area(predictions)\n",
    "    \n",
    "    return consensus,uncertainty,meanArea,stdArea,mpDsc,gDsc,mpIou,gIou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine how adjusting the dropout rate impacts on model performance - this can work in both directions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ious(dropoutRate,N):\n",
    "    \n",
    "    model = MultiResUnet(height=X.shape[1],\n",
    "                                width=X.shape[2],\n",
    "                                n_channels=1,\n",
    "                                layer_dropout_rate=None,\n",
    "                                block_dropout_rate=dropoutRate,\n",
    "                               )\n",
    "    \n",
    "    model.load_weights(modelParamFile)    \n",
    "\n",
    "    pred,uncertainty,meanArea,stdArea,mpDsc,gDsc,mpIouPred,gIou = map(np.array,zip(*[predict_stochastic(model=model,N=N,X=x.reshape(1,208,208,1)) for x in X_test]))\n",
    "\n",
    "    pred = pred.reshape(*X_test.shape)\n",
    "    \n",
    "    IOU = np.array([iou(Y_test[m,:,:,:], pred[m,:,:]) for m in range(MTest)])\n",
    "    \n",
    "    return IOU\n",
    "\n",
    "dropRates = np.array(sorted([0,10**-3,10**-2.5, 10**-2, 10**-1.5, 10**-1, 10**-0.5, 0.5,originalDropoutRate]))\n",
    "nRates = len(dropRates)\n",
    "ious = np.zeros((dropRates.size,MTest))\n",
    "\n",
    "for ind,rate in enumerate(dropRates):\n",
    "    clear_output()\n",
    "    print('/'.join((str(ind+1),str(nRates))))\n",
    "    \n",
    "    ious[ind,:] = get_ious(rate,20)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the original model performance..\n",
    "modelHistory = pd.read_csv(os.path.join('data','models','model_history.csv'),index_col=0)\n",
    "originalMeanIOU = modelHistory.loc[modelBaseName,'TestIOUMean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotX = np.log10(dropRates.copy())\n",
    "plotX[0] = -5\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.axhline(originalMeanIOU,c = 'r',linestyle='--')\n",
    "plt.axvline(np.log10(originalDropoutRate),c='r',linestyle='--')\n",
    "\n",
    "plt.errorbar(plotX,ious.mean(axis=1),ious.std(axis=1))\n",
    "xlabels = list(plotX)\n",
    "xlabels[0] = 'no dropout'\n",
    "plt.xticks(plotX,xlabels)\n",
    "\n",
    "\n",
    "plt.xlabel('log dropout rate')\n",
    "plt.ylabel('IOU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there is very little effect on performance from lowering the dropout rate relative to the (as expected given what dropout is usually used for). But, what happens with indivdual examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "plt.plot([0,1],[0,1],c='k')\n",
    "\n",
    "plt.scatter(ious[0,:],ious[6,:])\n",
    "\n",
    "plt.axis('square')\n",
    "\n",
    "plt.xlabel('iou (no dropout)')\n",
    "plt.ylabel('iou (original dropout rate)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are some small changes but nothing major. Lets go onto examining how accuracy predictions vary with INCREASING dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_values(dropoutRate,N):\n",
    "    \n",
    "    model = MultiResUnet(height=X.shape[1],\n",
    "                                width=X.shape[2],\n",
    "                                n_channels=1,\n",
    "                                layer_dropout_rate=None,\n",
    "                                block_dropout_rate=dropoutRate,\n",
    "                               )\n",
    "    \n",
    "    model.load_weights(modelParamFile)\n",
    "\n",
    "    pred,uncertainty,meanArea,stdArea,mpDsc,gDsc,mpIouPred,gIou = map(np.array,zip(*[predict_stochastic(model=model,N=N,X=x.reshape(1,208,208,1)) for x in X_test]))\n",
    "    \n",
    "    return mpDsc,gDsc,mpIouPred,gIou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictModel = MultiResUnet(height=X.shape[1],\n",
    "                            width=X.shape[2],\n",
    "                            n_channels=1,\n",
    "                            layer_dropout_rate=None,\n",
    "                            block_dropout_rate=originalDropoutRate,\n",
    "                           )\n",
    "\n",
    "predictModel.load_weights(modelParamFile)\n",
    "\n",
    "predPred,uncertaintyPred,meanAreaPred,stdAreaPred,mpDscPred,gDscPred,mpIouPred,gIouPred = map(np.array,zip(*[predict_stochastic(model=predictModel,N=20,X=x.reshape(1,208,208,1)) for x in X_test]))\n",
    "del predictModel\n",
    "\n",
    "predPred = predPred.reshape(*X_test.shape)\n",
    "\n",
    "#all true IOU and DSC fpr the predictions.\n",
    "IOU = [iou(Y_test[m,:,:,:], predPred[m,:,:]) for m in range(MTest)]\n",
    "DSC = [dsc(Y_test[m,:,:,:], predPred[m,:,:]) for m in range(MTest)]\n",
    "\n",
    "\n",
    "\n",
    "trues = [IOU,DSC]\n",
    "\n",
    "metricNames = ['mean pairwise Dice coefficient',\n",
    "               'global Dice coefficient',\n",
    "               'mean pairwise IOU',\n",
    "               'global IOU'\n",
    "              ]\n",
    "\n",
    "trueNames = ['true IOU','true DSC']\n",
    "\n",
    "names = ['/'.join((m,t)) for t,m in itertools.product(trueNames,metricNames)]\n",
    "\n",
    "nCombs = len(names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how good are the predicted metrics generated concurrently with the segementations? This represents a comparison with varying the rate\n",
    "\n",
    "originals = [mpDscPred,gDscPred,mpIouPred,gIouPred]\n",
    "\n",
    "rOriginal = np.zeros(nCombs)\n",
    "maeOriginal =np.zeros(nCombs)\n",
    "\n",
    "for combInd,(t,m) in enumerate(itertools.product(trues,originals)):\n",
    "        \n",
    "    rOriginal[combInd] = pearsonr(t,m)[0]\n",
    "    maeOriginal[combInd] = np.mean(np.abs(m-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increaseDropRates = np.arange(originalDropoutRate,0.525,0.025)\n",
    "\n",
    "nRates = len(increaseDropRates)\n",
    "\n",
    "nRepeats = 5\n",
    "\n",
    "#initialise outputs for r and mae\n",
    "\n",
    "r = np.zeros((nCombs,nRates,nRepeats))\n",
    "mae = np.zeros((nCombs,nRates,nRepeats))\n",
    "\n",
    "for rateInd,rate in enumerate(increaseDropRates):\n",
    "\n",
    "    clear_output()\n",
    "    print('/'.join((str(rateInd+1),str(nRates))))\n",
    "    \n",
    "    for repeat in range(nRepeats):\n",
    "\n",
    "        mpDsc,gDsc,mpIou,gIou = get_metric_values(rate,20)\n",
    "\n",
    "        metrics = [mpDsc,gDsc,mpIou,gIou]\n",
    "\n",
    "        for combInd,(t,m) in enumerate(itertools.product(trues,metrics)):\n",
    "\n",
    "            r[combInd,rateInd,repeat] = pearsonr(t,m)[0]\n",
    "            mae[combInd,rateInd,repeat] = np.mean(np.abs(m-t))\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "for combInd,name in enumerate(names):       \n",
    "    l = plt.errorbar(increaseDropRates,r[combInd].mean(axis=-1),r[combInd].std(axis=-1),label= name)\n",
    "    plt.axhline(rOriginal[combInd],linestyle='--',c=l[0].get_c())\n",
    "plt.ylabel('pearson r')\n",
    "    \n",
    "    \n",
    "plt.subplot(3,1,2)\n",
    "for combInd,name in enumerate(names):     \n",
    "    l = plt.errorbar(increaseDropRates,mae[combInd].mean(axis=-1),mae[combInd].std(axis=-1),label= name)\n",
    "    plt.axhline(maeOriginal[combInd],linestyle='--',c=l[0].get_c())\n",
    "plt.ylabel('MAE')\n",
    "\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "mult = (1-mae)*r\n",
    "multOriginal = rOriginal*(1-maeOriginal)\n",
    "\n",
    "for combInd,name in enumerate(names):     \n",
    "    l = plt.errorbar(increaseDropRates,mult[combInd].mean(axis=-1),mult[combInd].std(axis=-1),label= name)\n",
    "    plt.axhline(multOriginal[combInd],linestyle='--',c=l[0].get_c())\n",
    "plt.ylabel('r * (1-MAE)')\n",
    "\n",
    "plt.xlabel('resampled dropout rate')\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, interestingly, there is no difference in predictive performance between the sample used to predict and the sample used to quantify that prediction - original mae and r are no different from that using an independent sample with the same dropout rate.\n",
    "\n",
    "Any gains in MAE/R from increasing dropout rate are extremely marginal if present at all (error bars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0 GPU",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
