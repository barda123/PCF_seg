{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is concerned with using a model (created by unet_dev.ipynb or mrunet_dev.ipynb) to predict the areas of pericardial fat in a (much!) larger subsample of the UKBiobank dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mask_utils import load_image,resample_image,pad_voxels\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "import os\n",
    "\n",
    "from network_utils import gpu_memory_limit,predict_stochastic\n",
    "from MultiResUNet.MultiResUNet import MultiResUnet\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import re\n",
    "\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pydicom as dcm\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit how much GPU RAM can be allocated by this notebook... 8GB is 1/3 of available\n",
    "gpu_memory_limit(6000)\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    PADSIZE = np.array(pickle.load(open(os.path.join('data','PADSIZE.pickle'),'rb')))\n",
    "    PXSPACING = np.array(pickle.load(open(os.path.join('data','PXSPACING.pickle'),'rb')))\n",
    "except:\n",
    "    print('PICKLES NOT FOUND OR UNREADABLE. try running extract_dcm_for_wsx.ipynb')\n",
    "PXAREA = np.product(PXSPACING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify which model to use\n",
    "modelBaseName = 'mrunet_bayesian_2020-07-13_13-40' \n",
    "\n",
    "#location of the actual saved model\n",
    "modelBaseName = os.path.join('data','models',modelBaseName)\n",
    "\n",
    "modelParamFile = modelBaseName + '.h5'\n",
    "modelArchitecture = modelBaseName + '.json'\n",
    "\n",
    "# with open( modelArchitecture , 'r') as json_file: # this has rotted due to tf upgrades\n",
    "#     MODEL = model_from_json( json_file.read() )\n",
    "    \n",
    "MODEL = MultiResUnet(height=PADSIZE[0],\n",
    "                     width=PADSIZE[1],\n",
    "                     n_channels=1,\n",
    "                     layer_dropout_rate=None,\n",
    "                     block_dropout_rate=0.25 #extarcted from json file which is now unreadable...\n",
    "                    )\n",
    "\n",
    "MODEL.load_weights(modelParamFile)\n",
    "\n",
    "#hyperparameter N, defined according to quantify_model_performance.ipynb\n",
    "N = 15\n",
    "\n",
    "accuracyModelPath = modelBaseName + '_prediction_conversion.pickle'\n",
    "ACCURACYMODEL = pickle.load(open(accuracyModelPath,'rb'))\n",
    "#IF THIS BREAKS IN FUTURE:\n",
    "#coefficient is 1.63920111\n",
    "#intercept is -0.66730187\n",
    "\n",
    "#file for writing results\n",
    "OUTPUT_DIRECTORY = os.path.join('data','pericardial','UKB_segmentations_for_Esmeralda')\n",
    "RESULTSFILE = os.path.join(OUTPUT_DIRECTORY,'UKB_pericardial_fat_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the details for image preprocessing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of all LAX zipfiles   \n",
    "allZips = sorted(glob.glob(os.path.join('data','imaging_by_participant','**','*_longaxis.zip'),recursive = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manifest(zipfileObject):\n",
    "    \n",
    "    allFiles= zipfileObject.namelist()\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tempDir:\n",
    "        reg = re.compile('manifest*')\n",
    "        manifestFiles = [f for f in allFiles if reg.match(f)]\n",
    "\n",
    "        if len(manifestFiles) != 1:\n",
    "            print('no manifest found')\n",
    "            return None\n",
    "        else:\n",
    "            zipfileObject.extract(manifestFiles[0],path=tempDir)\n",
    "            manifest = pd.read_csv(os.path.join(tempDir,manifestFiles[0]) , index_col=False)\n",
    "            \n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_image_in_series(zipfileObject,listOfDicomFiles):\n",
    "    \n",
    "    #first sort the list, as usually the one with the lowest trigger time is also the first one after sorting\n",
    "    sortedList = np.sort(listOfDicomFiles)\n",
    "        \n",
    "    with tempfile.TemporaryDirectory() as tempDir:\n",
    "        for dicom in sortedList:\n",
    "            zipfileObject.extract(dicom,path=tempDir)\n",
    "            triggerTime = dcm.read_file(os.path.join(tempDir,dicom)).TriggerTime\n",
    "            if triggerTime == 0.0:\n",
    "                return dicom\n",
    "\n",
    "def extract_first_4Ch_image(zipfilePath):\n",
    "    \n",
    "    '''THIS VERSION DOES NO CHECKS!!!!! IT JUST TRIES TO LOAD THE FIRST FILE'''\n",
    "    \n",
    "    zipfileObject = zipfile.ZipFile(zipfilePath)\n",
    "\n",
    "    manifest = get_manifest(zipfileObject)\n",
    "\n",
    "    #index for 4ch images\n",
    "    Index4Ch = (manifest =='CINE_segmented_LAX_4Ch').max(axis=1)\n",
    "    \n",
    "    \n",
    "    if not Index4Ch.any():\n",
    "        #if nothing labelled as a 4-chamber image, return nothing\n",
    "        return None,None,None\n",
    "\n",
    "    else: #if there *are* images labelled as 4Ch\n",
    "        with tempfile.TemporaryDirectory() as tempDir:\n",
    "            #get only the 4Chamber ones\n",
    "            manifest = manifest.loc[Index4Ch,:]\n",
    "            #separate the series\n",
    "            series = manifest.groupby(['series discription','seriesid'])\n",
    "            #get the date\n",
    "            imagedDate = manifest['date'].iloc[0]\n",
    "\n",
    "            if series.count().shape[0] == 1:    \n",
    "                #if there is only one series used, then get the first image from that one.\n",
    "                firstDicom = first_image_in_series(zipfileObject,manifest['filename'].values)\n",
    "                zipfileObject.extract(firstDicom,path=tempDir)\n",
    "            \n",
    "            else: \n",
    "#                 print('more than one series found...')\n",
    "                #if there is more than one series, do some logic\n",
    "\n",
    "                #filter for number of images - should be exactly 50\n",
    "                manifest  = series.filter(lambda x: x.count().max() == 50)\n",
    "\n",
    "                #get all the first dicoms...\n",
    "                firstDicoms = manifest.groupby(['series discription','seriesid']).apply(lambda x: first_image_in_series(zipfileObject,x['filename']))\n",
    "\n",
    "                #get the series times out...\n",
    "                firstDicoms.apply(lambda x: zipfileObject.extract(x,path=tempDir))\n",
    "                seriesTime  = firstDicoms.apply(lambda x: dcm.read_file(os.path.join(tempDir,x)).SeriesTime)\n",
    "\n",
    "                #and extract the latest one (assuming it will be better...)\n",
    "                firstDicom = firstDicoms.values[ np.argmax(seriesTime.values) ]\n",
    "\n",
    "            try:\n",
    "                dicom_location = os.path.join(tempDir,firstDicom)\n",
    "                image,spacing,dicom_object = load_image(dicomPath=dicom_location,desiredPxSpacing=PXSPACING, padSize=PADSIZE,return_dicom_object=True)\n",
    "                return image,imagedDate,dicom_object\n",
    "            except:\n",
    "                return None,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affine(d):\n",
    "    # Copyright 2017.\n",
    "    # Author: Wenjia Bai, Biomedical Image Analysis Group, Imperial College London.\n",
    "    #\n",
    "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "    # you may not use this file except in compliance with the License.\n",
    "    # You may obtain a copy of the License at\n",
    "    #\n",
    "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
    "    #\n",
    "    # Unless required by applicable law or agreed to in writing, software\n",
    "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "    # See the License for the specific language governing permissions and\n",
    "    # limitations under the License.\n",
    "    # ==============================================================================\n",
    "    X = d.Columns\n",
    "    Y = d.Rows\n",
    "    Z =1 # because in this context all images are 2D\n",
    "\n",
    "    dx = float(d.PixelSpacing[1])\n",
    "    dy = float(d.PixelSpacing[0])\n",
    "\n",
    "    # DICOM coordinate (LPS)\n",
    "    #  x: left\n",
    "    #  y: posterior\n",
    "    #  z: superior\n",
    "    # Nifti coordinate (RAS)\n",
    "    #  x: right\n",
    "    #  y: anterior\n",
    "    #  z: superior\n",
    "    # Therefore, to transform between DICOM and Nifti, the x and y coordinates need to be negated.\n",
    "    # Refer to\n",
    "    # http://nifti.nimh.nih.gov/pub/dist/src/niftilib/nifti1.h\n",
    "    # http://nifti.nimh.nih.gov/nifti-1/documentation/nifti1fields/nifti1fields_pages/figqformusage\n",
    "\n",
    "    # The coordinate of the upper-left voxel of the first and second slices\n",
    "    pos_ul = np.array([float(x) for x in d.ImagePositionPatient])\n",
    "    pos_ul[:2] = -pos_ul[:2]\n",
    "\n",
    "    # Image orientation\n",
    "    axis_x = np.array([float(x) for x in d.ImageOrientationPatient[:3]])\n",
    "    axis_y = np.array([float(x) for x in d.ImageOrientationPatient[3:]])\n",
    "    axis_x[:2] = -axis_x[:2]\n",
    "    axis_y[:2] = -axis_y[:2]\n",
    "\n",
    "    if Z >= 2:\n",
    "        # Read a dicom file at the second slice\n",
    "        d2 = dicom.read_file(os.path.join(dir[1], sorted(os.listdir(dir[1]))[0]))\n",
    "        pos_ul2 = np.array([float(x) for x in d2.ImagePositionPatient])\n",
    "        pos_ul2[:2] = -pos_ul2[:2]\n",
    "        axis_z = pos_ul2 - pos_ul\n",
    "        axis_z = axis_z / np.linalg.norm(axis_z)\n",
    "    else:\n",
    "        axis_z = np.cross(axis_x, axis_y)\n",
    "\n",
    "    # Determine the z spacing\n",
    "    if hasattr(d, 'SpacingBetweenSlices'):\n",
    "        dz = float(d.SpacingBetweenSlices)\n",
    "    elif Z >= 2:\n",
    "        print('Warning: can not find attribute SpacingBetweenSlices. Calculate from two successive slices.')\n",
    "        dz = float(np.linalg.norm(pos_ul2 - pos_ul))\n",
    "    else:\n",
    "        print('Warning: can not find attribute SpacingBetweenSlices. Use attribute SliceThickness instead.')\n",
    "        dz = float(d.SliceThickness)\n",
    "\n",
    "    # Affine matrix which converts the voxel coordinate to world coordinate\n",
    "    affine = np.eye(4)\n",
    "    affine[:3,0] = axis_x * dx\n",
    "    affine[:3,1] = axis_y * dy\n",
    "    affine[:3,2] = axis_z * dz\n",
    "    affine[:3,3] = pos_ul\n",
    "    \n",
    "    return affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXMEEEE update this list to match the output arguments in network_utils/predict_stochastic\n",
    "# RESNAMES = ['consensus','uncertainty','meanArea (mm2)','stdArea (mm2)','mpDsc','gDsc','mpIou','gIou']\n",
    "RESNAMES = ['meanArea (cm2)','stdArea (cm2)','predicted DSC']\n",
    "\n",
    "def get_feid(zipfilePath):\n",
    "    return os.path.basename(zipfilePath)[:7]\n",
    "\n",
    "def quantify_fat(zipfilePath):\n",
    "    \n",
    "    feid = get_feid(zipfilePath)\n",
    "    #create dictionary for returning results.\n",
    "    resultDict = {'f.eid':feid}\n",
    "    \n",
    "    #extract the pixels for each image.\n",
    "    im,imagedDate,dicom_object = extract_first_4Ch_image(zipfilePath)\n",
    "    \n",
    "    if im is not None:\n",
    "        resultDict['date'] = imagedDate\n",
    "        im = im.reshape((1,*im.shape,1))\n",
    "        res = predict_stochastic(MODEL,N,ACCURACYMODEL,im) #FIXMEE remove the unnecessary metrics\n",
    "        \n",
    "        #get the actual predicted segmentation (boolean version)\n",
    "        segmentation = res[0].squeeze()\n",
    "        \n",
    "        #resize to original resolution and dimensions\n",
    "        segmentation = resample_image(segmentation,PXSPACING,np.array(dicom_object.PixelSpacing))\n",
    "        segmentation = pad_voxels(segmentation,dicom_object.pixel_array.squeeze().shape)\n",
    "                \n",
    "        assert all([s==o for s,o in zip(segmentation.shape,dicom_object.pixel_array.shape)]),\"you've broken something in the image dimensions\"\n",
    "        \n",
    "        #output directory for segmentation\n",
    "        output_directory = os.path.join(OUTPUT_DIRECTORY,str(feid))\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "        \n",
    "        #locations of the output nifti files...\n",
    "        img_loc = os.path.join(output_directory,'la_4ch_ED.nii.gz')\n",
    "        seg_loc = os.path.join(output_directory,'pcf_seg_la_4ch_ED.nii.gz')\n",
    "        \n",
    "        #make niftis from dicom object...\n",
    "        vol, pixdim, affine = dn.dicom_to_volume([dicom_object])        \n",
    "        segmentation = segmentation.reshape(*vol.shape).astype('int')\n",
    "        affine = get_affine(dicom_object)\n",
    "        \n",
    "        #convert coordinares and write nifti for image\n",
    "        dn.write_nifti(img_loc, vol, affine)\n",
    "        vol, pixdim, _ = dn.dicom_to_volume([dicom_object])\n",
    "        \n",
    "        assert all([s==o for s,o in zip(segmentation.shape,vol.shape)]),\"you've broken something in the image dimensions\"\n",
    "        dn.write_nifti(seg_loc, segmentation, affine)        \n",
    "        \n",
    "        #wrap quantitative results up into a dict for easy DataFram-ing\n",
    "        resultDict.update(dict(zip(RESNAMES,res[2:])))\n",
    "\n",
    "        #ensure that units of area are correct...\n",
    "        resultDict['meanArea (cm2)'] *= (PXAREA/100)\n",
    "        resultDict['stdArea (cm2)'] *= (PXAREA/100)\n",
    "        return resultDict\n",
    "    else:\n",
    "        return resultDict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(RESULTSFILE):\n",
    "    results = pd.read_csv(RESULTSFILE)\n",
    "else:\n",
    "    #create a dataframe to store results\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "for i,zipfilePath in enumerate(allZips):\n",
    "    if i not in results.index:\n",
    "        result = quantify_fat(zipfilePath)\n",
    "        results = results.append(result,ignore_index=True)\n",
    "    if i % 100 == 0:\n",
    "        results.to_csv(RESULTSFILE,index=False)\n",
    "\n",
    "results.to_csv(RESULTSFILE,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
