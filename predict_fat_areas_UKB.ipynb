{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is concerned with using a model (created by unet_dev.ipynb or mrunet_dev.ipynb) to predict the areas of pericardial fat in a (much!) larger subsample of the UKBiobank dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mask_utils import load_image\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "import os\n",
    "\n",
    "from network_utils import gpu_memory_limit,predict_stochastic\n",
    "from MultiResUNet.MultiResUNet import MultiResUnet\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import re\n",
    "\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pydicom as dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit how much GPU RAM can be allocated by this notebook... 8GB is 1/3 of available\n",
    "# gpu_memory_limit(6000)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "modelBaseName = 'mrunet_bayesian_2020-07-13_13:40' \n",
    "\n",
    "#location of the actual saved model\n",
    "modelBaseName = os.path.join('data','models',modelBaseName)\n",
    "\n",
    "modelParamFile = modelBaseName + '.h5'\n",
    "modelArchitecture = modelBaseName + '.json'\n",
    "\n",
    "with open( modelArchitecture , 'r') as json_file:\n",
    "    MODEL = model_from_json( json_file.read() )\n",
    "\n",
    "MODEL.load_weights(modelParamFile)\n",
    "\n",
    "#hyperparameter N, defined according to quantify_model_performance.ipynb\n",
    "N = 15\n",
    "\n",
    "\n",
    "accuracyModelPath = modelBaseName + '_prediction_conversion.pickle'\n",
    "ACCURACYMODEL = pickle.load(open(accuracyModelPath,'rb'))\n",
    "\n",
    "#file for writing results\n",
    "RESULTSFILE = os.path.join('data','UKB_pericardial_fat_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the details for image preprocessing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADSIZE = pickle.load(open(os.path.join('data','PADSIZE.pickle'),'rb'))\n",
    "PXSPACING = pickle.load(open(os.path.join('data','PXSPACING.pickle'),'rb'))\n",
    "PXAREA = np.product(PXSPACING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of all LAX zipfiles   \n",
    "allZips = glob.glob(os.path.join('data','imaging_by_participant','**','*_longaxis.zip'),recursive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manifest(zipfileObject):\n",
    "    \n",
    "    allFiles= zipfileObject.namelist()\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tempDir:\n",
    "        reg = re.compile('manifest*')\n",
    "        manifestFiles = [f for f in allFiles if reg.match(f)]\n",
    "\n",
    "        if len(manifestFiles) != 1:\n",
    "            print('no manifest found')\n",
    "            return None\n",
    "        else:\n",
    "            zipfileObject.extract(manifestFiles[0],path=tempDir)\n",
    "            manifest = pd.read_csv(os.path.join(tempDir,manifestFiles[0]) , index_col=False)\n",
    "            \n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_image_in_series(zipfileObject,listOfDicomFiles):\n",
    "    \n",
    "    #first sort the list, as usually the one with the lowest trigger time is also the first one after sorting\n",
    "    sortedList = np.sort(listOfDicomFiles)\n",
    "        \n",
    "    with tempfile.TemporaryDirectory() as tempDir:\n",
    "        for dicom in sortedList:\n",
    "            zipfileObject.extract(dicom,path=tempDir)\n",
    "            triggerTime = dcm.read_file(os.path.join(tempDir,dicom)).TriggerTime\n",
    "            if triggerTime == 0.0:\n",
    "                return dicom\n",
    "\n",
    "def extract_first_4Ch_image(zipfilePath):\n",
    "    \n",
    "    '''THIS VERSION DOES NO CHECKS!!!!! IT JUST TRIES TO LOAD THE FIRST FILE'''\n",
    "    \n",
    "    \n",
    "    zipfileObject = zipfile.ZipFile(zipfilePath)\n",
    "\n",
    "    manifest = get_manifest(zipfileObject)\n",
    "\n",
    "    #index for 4ch images\n",
    "    Index4Ch = (manifest =='CINE_segmented_LAX_4Ch').max(axis=1)\n",
    "    \n",
    "    \n",
    "    if not Index4Ch.any():\n",
    "        #if nothing labelled as a 4-chamber image, return nothing\n",
    "        return None,None\n",
    "\n",
    "    else: #if there *are* images labelled as 4Ch\n",
    "        with tempfile.TemporaryDirectory() as tempDir:\n",
    "            #get only the 4Chamber ones\n",
    "            manifest = manifest.loc[Index4Ch,:]\n",
    "            #separate the series\n",
    "            series = manifest.groupby(['series discription','seriesid'])\n",
    "            #get the date\n",
    "            imagedDate = manifest['date'].iloc[0]\n",
    "\n",
    "            if series.count().shape[0] == 1:    \n",
    "                #if there is only one series used, then get the first image from that one.\n",
    "                firstDicom = first_image_in_series(zipfileObject,manifest['filename'].values)\n",
    "                zipfileObject.extract(firstDicom,path=tempDir)\n",
    "            \n",
    "            else: \n",
    "#                 print('more than one series found...')\n",
    "                #if there is more than one series, do some logic\n",
    "\n",
    "                #filter for number of images - should be exactly 50\n",
    "                manifest  = series.filter(lambda x: x.count().max() == 50)\n",
    "\n",
    "                #get all the first dicoms...\n",
    "                firstDicoms = manifest.groupby(['series discription','seriesid']).apply(lambda x: first_image_in_series(zipfileObject,x['filename']))\n",
    "\n",
    "                #get the series times out...\n",
    "                firstDicoms.apply(lambda x: zipfileObject.extract(x,path=tempDir))\n",
    "                seriesTime  = firstDicoms.apply(lambda x: dcm.read_file(os.path.join(tempDir,x)).SeriesTime)\n",
    "\n",
    "                #and extract the latest one (assuming it will be better...)\n",
    "                firstDicom = firstDicoms.values[ np.argmax(seriesTime.values) ]\n",
    "\n",
    "            try:\n",
    "                image = load_image(dicomPath=os.path.join(tempDir,firstDicom),desiredPxSpacing=PXSPACING, padSize=PADSIZE)\n",
    "                return image[0],imagedDate\n",
    "            except:\n",
    "                return None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXMEEEE update this list to match the output arguments in network_utils/predict_stochastic\n",
    "# RESNAMES = ['consensus','uncertainty','meanArea (mm2)','stdArea (mm2)','mpDsc','gDsc','mpIou','gIou']\n",
    "RESNAMES = ['meanArea (cm2)','stdArea (cm2)','predicted DSC']\n",
    "\n",
    "def get_feid(zipfilePath):\n",
    "    return os.path.basename(zipfilePath)[:7]\n",
    "\n",
    "def quantify_fat(zipfilePath):\n",
    "    \n",
    "    feid = get_feid(zipfilePath)\n",
    "    #create dictionary for returning results.\n",
    "    resultDict = {'f.eid':feid}\n",
    "    \n",
    "    #extract the pixels for each image.\n",
    "    im,imagedDate = extract_first_4Ch_image(zipfilePath)\n",
    "    \n",
    "    if im is not None:\n",
    "        resultDict['date'] = imagedDate\n",
    "        im = im.reshape((1,*im.shape,1))\n",
    "        res = predict_stochastic(MODEL,N,ACCURACYMODEL,im) #FIXMEE remove the unnecessary metrics\n",
    "\n",
    "        #wrap up into a dict for easy DataFram-ing\n",
    "        resultDict.update(dict(zip(RESNAMES,res[2:])))\n",
    "\n",
    "        #ensure that units of area are correct...\n",
    "        resultDict['meanArea (cm2)'] *= (PXAREA/100)\n",
    "        resultDict['stdArea (cm2)'] *= (PXAREA/100)\n",
    "        return resultDict\n",
    "    else:\n",
    "        return resultDict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe to store results\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for zipfilePath in allZips:\n",
    "#     if get_feid(zipfilePath) not in results.index:\n",
    "    result = quantify_fat(zipfilePath)\n",
    "    results = results.append(result,ignore_index=True)\n",
    "        \n",
    "results.to_csv(RESULTSFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0 GPU",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
