{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as keras\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from custom_losses import binary_crossentropy_weight_balance, binary_crossentropy_weight_dict, binary_crossentropy_closeness_to_foreground,dice_coef_loss\n",
    "\n",
    "from mask_utils import show_image_with_masks\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_memory_limit(memory_limit):\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        # Restrict TensorFlow to only allocate 16GB of memory on the first GPU\n",
    "        try:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit)])\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "            print('GPU memory limit allocated.')\n",
    "        except RuntimeError as e:\n",
    "            # Virtual devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "            \n",
    "gpu_memory_limit(5000) # 8GB is 1/3 of available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory for keeping models, and journaling their performance/graphs\n",
    "modelDir = os.path.join('data','models')\n",
    "if not os.path.isdir(modelDir):\n",
    "    os.mkdir(modelDir)\n",
    "    \n",
    "dateStr = str(date.today())\n",
    "outputName = os.path.join(modelDir,'unet_' + dateStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataDir = './data/pericardial/wsx_round2/'\n",
    "\n",
    "#load data - these files created by extract_dcm_for_wsx.ipynb\n",
    "X = np.load(os.path.join(DataDir,'X.npy'))\n",
    "Y = np.load(os.path.join(DataDir,'Y.npy')).astype('float')\n",
    "pxSize = np.load(os.path.join(DataDir,'pxSize.npy'))\n",
    "\n",
    "#ensure the shape is correct arrays saved were rank 3, so this changes to rank 4 (last dimension represents channels)\n",
    "X = X.reshape([*X.shape,1])\n",
    "Y = Y.reshape([*Y.shape,1])\n",
    "\n",
    "\n",
    "\n",
    "#do train/test split!\n",
    "X, X_test, Y, Y_test,pxSize,pxSize_test = train_test_split(X, Y, pxSize, test_size=0.2,random_state=101)\n",
    "\n",
    "#\n",
    "M = X.shape[0]\n",
    "MTest = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class augmentImageSequence(Sequence):\n",
    "    \n",
    "    '''class for data augmentation on matched image/mask pairs'''\n",
    "    \n",
    "    def __init__(self,Images,Masks,dataGenArgs,batchSize=1,seed=42):\n",
    "        \n",
    "        #copy raw data in\n",
    "        self.x,self.y = Images,Masks\n",
    "        self.batch_size = batchSize\n",
    "        \n",
    "        #convert to imageDataGenerators/create flow objects...\n",
    "        self.augmentIm = ImageDataGenerator(**dataGenArgs).flow(x=Images,batch_size=batchSize,seed=seed)\n",
    "        self.augmentMa = ImageDataGenerator(**dataGenArgs).flow(x=Masks, batch_size=batchSize,seed=seed)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        #cheaty fake 1-stage loop, returns 1 batch from both flow objects (which will be matched)\n",
    "        for _,ims,masks in zip(range(1),self.augmentIm,self.augmentMa):        \n",
    "            \n",
    "            masks = (masks>0.5).astype('float')\n",
    "            \n",
    "            return ims,masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-net architecture...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(pretrained_weights = None,input_size = (256,256,1),dropoutRate = 0):\n",
    "    inputs = layers.Input(input_size)\n",
    "    conv1 = layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(inputs)\n",
    "    conv1 = layers.Dropout(rate=dropoutRate)(conv1)\n",
    "    conv1 = layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv1)\n",
    "    conv1 = layers.Dropout(rate=dropoutRate)(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(pool1)\n",
    "    conv2 = layers.Dropout(rate=dropoutRate)(conv2)\n",
    "    conv2 = layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv2)\n",
    "    conv2 = layers.Dropout(rate=dropoutRate)(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(pool2)\n",
    "    conv3 = layers.Dropout(rate=dropoutRate)(conv3)\n",
    "    conv3 = layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv3)\n",
    "    conv3 = layers.Dropout(rate=dropoutRate)(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(pool3)\n",
    "    conv4 = layers.Dropout(rate=dropoutRate)(conv4)\n",
    "    conv4 = layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv4)\n",
    "    conv4 = layers.Dropout(rate=dropoutRate)(conv4)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = layers.Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(pool4)\n",
    "    conv5 = layers.Dropout(rate=dropoutRate)(conv5)\n",
    "    conv5 = layers.Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv5)\n",
    "    conv5 = layers.Dropout(rate=dropoutRate)(conv5)\n",
    "\n",
    "    up6 = layers.Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(layers.UpSampling2D(size = (2,2))(conv5))\n",
    "    merge6 = layers.concatenate([conv4,up6], axis = 3)\n",
    "    conv6 = layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(merge6)\n",
    "    conv6 = layers.Dropout(rate=dropoutRate)(conv6)\n",
    "    conv6 = layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv6)\n",
    "    conv6 = layers.Dropout(rate=dropoutRate)(conv6)\n",
    "\n",
    "    up7 = layers.Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(layers.UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = layers.concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(merge7)\n",
    "    conv7 = layers.Dropout(rate=dropoutRate)(conv7)\n",
    "    conv7 = layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv7)\n",
    "    conv7 = layers.Dropout(rate=dropoutRate)(conv7)\n",
    "\n",
    "    up8 = layers.Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(layers.UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = layers.concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(merge8)\n",
    "    conv8 = layers.Dropout(rate=dropoutRate)(conv8)\n",
    "    conv8 = layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv8)\n",
    "    conv8 = layers.Dropout(rate=dropoutRate)(conv8)\n",
    "\n",
    "    up9 = layers.Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(layers.UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = layers.concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(merge9)\n",
    "    conv9 = layers.Dropout(rate=dropoutRate)(conv9)\n",
    "    conv9 = layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv9)\n",
    "    conv9 = layers.Dropout(rate=dropoutRate)(conv9)\n",
    "    conv9 = layers.Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv9)\n",
    "    conv9 = layers.Dropout(rate=dropoutRate)(conv9)\n",
    "    conv10 = layers.Conv2D(1, 1, activation = 'sigmoid',kernel_initializer = 'glorot_normal',bias_initializer = initializers.Constant(0.01))(conv9)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = conv10)    \n",
    "    #model.summary()\n",
    "\n",
    "    if (pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#properties for data augmentation\n",
    "dataGenArgs = dict(rotation_range=5,\n",
    "                   width_shift_range=0.05,\n",
    "                   height_shift_range=0.05,\n",
    "                   shear_range=0,#0.05,\n",
    "                   zoom_range=0.05,\n",
    "                   horizontal_flip=False, #DO NOT FLIP THE IMAGES FFS\n",
    "                   vertical_flip=False,\n",
    "                   fill_mode='nearest',\n",
    "                   data_format= 'channels_last',\n",
    "                   featurewise_center=False,\n",
    "                   featurewise_std_normalization=False,\n",
    "                   zca_whitening=False,\n",
    "                  )\n",
    "\n",
    "\n",
    "earlyStop = callbacks.EarlyStopping(patience=6, #be a bit patient...\n",
    "                                    min_delta=0,\n",
    "                                    monitor='loss',\n",
    "                                    restore_best_weights=True,\n",
    "                                    mode='min',\n",
    "                                   )\n",
    "\n",
    "reduceLR = callbacks.ReduceLROnPlateau(monitor='loss',\n",
    "                                       patience=5,\n",
    "                                       factor=0.3,\n",
    "                                       verbose=1,\n",
    "                                       cooldown=5,\n",
    "                                      )\n",
    "\n",
    "CALLBACKS = [earlyStop,\n",
    "             reduceLR\n",
    "            ]\n",
    "\n",
    "OPT = Adam(learning_rate = 3e-5,\n",
    "           beta_1 = 0.9,\n",
    "           beta_2 = 0.999,\n",
    "           amsgrad = False\n",
    "          )\n",
    "\n",
    "#calculate weights but over whole training set\n",
    "MULTIPLIER = Y.size/Y.sum()\n",
    "\n",
    "#other hyperparameters\n",
    "BATCHSIZE = 16 #THIS MATTERS A LOT\n",
    "DROPOUTRATE = 0\n",
    "WEIGHT_DICT = {0.:1.,1.:MULTIPLIER}\n",
    "\n",
    "#Spatial smoothing for pixel weights\n",
    "SIGMA = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.clear_session()\n",
    "\n",
    "tf.random.set_seed(101) #FIXME!!! this is not sufficient to guarantee deterministic behaviour during fitting.\n",
    "\n",
    "model = unet(input_size=X.shape[1:],dropoutRate=DROPOUTRATE)\n",
    "\n",
    "model.compile(optimizer = OPT, \n",
    "#               loss = 'binary_crossentropy',\n",
    "#               loss = binary_crossentropy_weight_balance,\n",
    "#               loss = binary_crossentropy_closeness_to_foreground(sigma=SIGMA),\n",
    "              loss = dice_coef_loss,\n",
    "              metrics = ['accuracy',metrics.MeanIoU(num_classes=2)],\n",
    "             )\n",
    "\n",
    "fitHistory = model.fit(augmentImageSequence(X,Y,dataGenArgs,batchSize=BATCHSIZE),\n",
    "                       epochs = 200,#think about me... \n",
    "                       steps_per_epoch= M//BATCHSIZE, #obvs\n",
    "                       workers=8,\n",
    "                       use_multiprocessing=True,\n",
    "                       validation_data=(X_test,Y_test.astype('float')),\n",
    "                       callbacks=CALLBACKS,\n",
    "                       verbose=1,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at how fitting has proceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(fitHistory.history['loss'],label = 'train')\n",
    "plt.plot(fitHistory.history['val_loss'],label = 'dev')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.xticks([])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(fitHistory.history['mean_io_u'],label = 'train')\n",
    "plt.plot(fitHistory.history['val_mean_io_u'],label = 'dev')\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel('mean iou')\n",
    "\n",
    "plt.xlabel('epoch #')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the values actually output by the model - as there is some instability atm this is pretty important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTest = model.predict(X_test)\n",
    "\n",
    "predTrain = model.predict(X)\n",
    "\n",
    "#show the actual distribution of output values\n",
    "plt.hist(predTrain.flatten(),density=True, alpha=0.5, label = 'Train',bins = np.arange(0,1.05,0.1))\n",
    "plt.hist(predTest.flatten(), density=True, alpha=0.5, label = 'Test' ,bins = np.arange(0,1.05,0.1),)\n",
    "# plt.title('distribution of output values over all pixels')\n",
    "plt.legend()\n",
    "plt.xlabel('value')\n",
    "plt.ylabel('probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the  distribution of IoU (rather than just the mean)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(yTrue,yPred):\n",
    "    '''intersection-over-union score'''\n",
    "    \n",
    "    yTrue = yTrue>=0.5\n",
    "    yPred = yPred>=0.5\n",
    "    \n",
    "    intersection = np.sum(np.logical_and(yTrue,yPred))\n",
    "    \n",
    "    union = np.sum(np.logical_or(yTrue,yPred))\n",
    "    \n",
    "    return intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over th eexample axis, calculating IoU for each image separately\n",
    "TrainIOU = [iou(Y[m,:,:,:], predTrain[m,:,:]) for m in range(MTest)]\n",
    "TestIOU = [iou(Y_test[m,:,:,:], predTest[m,:,:]) for m in range(MTest)]\n",
    "plt.hist(TrainIOU , bins = np.arange(0,1.05,0.05), density=True, alpha=0.5, label = 'Train')\n",
    "plt.hist(TestIOU ,  bins = np.arange(0,1.05,0.05), density=True, alpha=0.5, label = 'Test')\n",
    "\n",
    "plt.xlabel('iou')\n",
    "plt.ylabel('probability density')\n",
    "\n",
    "plt.legend()\n",
    " \n",
    "plt.savefig(outputName + '_iou_histogram.svg')\n",
    "plt.savefig(outputName + '_iou_histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do predicted **areas** of fat match? That is what the project is all about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "\n",
    "\n",
    "areasPredTrain = np.sum(predTrain,axis=(1,2,3)) * pxSize / 100\n",
    "areasTrueTrain = np.sum(Y,axis=(1,2,3)) * pxSize / 100\n",
    "\n",
    "areasPredTest = np.sum(predTest,axis=(1,2,3)) * pxSize_test / 100\n",
    "areasTrueTest = np.sum(Y_test,axis=(1,2,3)) * pxSize_test / 100\n",
    "\n",
    "plt.scatter(areasTrueTrain,areasPredTrain,label = 'train',alpha=0.6)\n",
    "plt.scatter(areasTrueTest,areasPredTest,label = 'test',alpha=0.6)\n",
    "\n",
    "r2,p = pearsonr(areasTrueTest,areasPredTest)\n",
    "\n",
    "plt.title('for test set, R$^2$ = ' + f'{r2:.03}' + ', p = ' + f'{p:.03}')\n",
    "\n",
    "plt.xlabel('human area (cm$^2$)')\n",
    "\n",
    "plt.ylabel('machine area (cm$^2$)')\n",
    "\n",
    "# plt.axis('equal')\n",
    "lims = [0,np.max(np.hstack((areasPredTrain,areasTrueTrain,areasPredTest,areasTrueTest)))]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(outputName + '_area_correlation.svg')\n",
    "plt.savefig(outputName + '_area_correlation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a few examples of the training set segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negs = 25\n",
    "\n",
    "egs = np.random.randint(M,size = negs)\n",
    "\n",
    "ncols = 5\n",
    "nrows = np.ceil(negs/ncols)\n",
    "\n",
    "plt.figure(figsize = (5*ncols,5*nrows))\n",
    "\n",
    "imShape = X.shape[1:-1]\n",
    "\n",
    "for i in range(negs):\n",
    "    \n",
    "    plt.subplot(nrows,ncols,i+1)\n",
    "    \n",
    "    manual,automated = Y[egs[i],:,:].reshape(imShape), predTrain[egs[i],:,:].reshape(imShape) > 0.5\n",
    "    \n",
    "    show_image_with_masks(image = X[egs[i],:,:].reshape(imShape),\n",
    "                          masks = [manual,automated],\n",
    "                          maskOptions = [{'linewidth':1,'color':'g'},{'linewidth':1,'color':'r'}]\n",
    "                         )\n",
    "    \n",
    "    plt.title('iou = ' + f'{iou(manual,automated):.03}')\n",
    "    \n",
    "plt.savefig(outputName + '_train_examples.svg')\n",
    "plt.savefig(outputName + '_train_examples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negs = 25\n",
    "\n",
    "egs = np.random.randint(MTest,size = negs)\n",
    "\n",
    "ncols = 5\n",
    "nrows = np.ceil(negs/ncols)\n",
    "\n",
    "plt.figure(figsize = (5*ncols,5*nrows))\n",
    "\n",
    "imShape = X_test.shape[1:-1]\n",
    "\n",
    "for i in range(negs):\n",
    "    \n",
    "    plt.subplot(nrows,ncols,i+1)\n",
    "    \n",
    "    manual,automated = Y_test[egs[i],:,:].reshape(imShape), predTest[egs[i],:,:].reshape(imShape) > 0.5\n",
    "    \n",
    "    show_image_with_masks(image = X[egs[i],:,:].reshape(imShape),\n",
    "                          masks = [manual,automated],\n",
    "                          maskOptions = [{'linewidth':1,'color':'g'},{'linewidth':1,'color':'r'}]\n",
    "                         )\n",
    "    \n",
    "    plt.title('iou = ' + f'{iou(manual,automated):.03}')\n",
    "    \n",
    "    \n",
    "plt.savefig(outputName + '_test_examples.svg')\n",
    "plt.savefig(outputName + '_test_examples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, save the model for use elsewhere, along with some performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#need to save architecture and weight separately as custom loss functions cause issues with loading from a single .h5\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open( outputName + '.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(outputName + '.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are likely to be a number of models written - but these are not all equal. Also write some metrics of model performance for the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format a line to add to the csv\n",
    "\n",
    "modelDetails = {'Filename': outputName,\n",
    "                'TrainIOUMean': str(np.mean(TrainIOU)),\n",
    "                'TrainIOUStd': str(np.std(TrainIOU)),\n",
    "                'TestIOUMean': str(np.mean(TrainIOU)),\n",
    "                'TestIOUStd': str(np.std(TrainIOU)), \n",
    "               }\n",
    "\n",
    "#if the file containing details of past models does not exist, then create it (with a header row)\n",
    "historyFile = os.path.join(modelDir,'model_history.csv')\n",
    "if not os.path.isfile(historyFile):\n",
    "    \n",
    "    fields = modelDetails.keys()\n",
    "    \n",
    "    with open(historyFile,'w+') as f:\n",
    "        f.write(','.join(fields) + '\\n')\n",
    "        \n",
    "#now write out the line of performance statistics.\n",
    "with open(historyFile,'a') as f:\n",
    "    f.write(','.join(modelDetails.values()) + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0 GPU",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
